{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTT: 國泰 保險: 100%|██████████| 1/1 [00:01<00:00,  1.50s/it]\n",
      "Mobile01: 國泰 保險: 0it [00:00, ?it/s]\n",
      "my83: 國泰 保險: 0it [00:00, ?it/s]\n",
      "finfo: 國泰 保險: 0it [00:00, ?it/s]\n",
      "PTT: 國泰人壽: 100%|██████████| 20/20 [00:29<00:00,  1.45s/it]\n",
      "Mobile01: 國泰人壽: 0it [00:00, ?it/s]\n",
      "my83: 國泰人壽: 0it [00:00, ?it/s]\n",
      "finfo: 國泰人壽: 0it [00:00, ?it/s]\n",
      "PTT: 保單: 100%|██████████| 20/20 [00:27<00:00,  1.39s/it]\n",
      "Mobile01: 保單: 0it [00:00, ?it/s]\n",
      "my83: 保單: 0it [00:00, ?it/s]\n",
      "finfo: 保單: 0it [00:00, ?it/s]\n",
      "PTT: 保險理賠: 100%|██████████| 20/20 [00:30<00:00,  1.51s/it]\n",
      "Mobile01: 保險理賠: 0it [00:00, ?it/s]\n",
      "my83: 保險理賠: 0it [00:00, ?it/s]\n",
      "finfo: 保險理賠: 0it [00:00, ?it/s]\n",
      "PTT: 保險詐騙: 100%|██████████| 1/1 [00:01<00:00,  1.77s/it]\n",
      "Mobile01: 保險詐騙: 0it [00:00, ?it/s]\n",
      "my83: 保險詐騙: 0it [00:00, ?it/s]\n",
      "finfo: 保險詐騙: 0it [00:00, ?it/s]\n",
      "PTT: 業務推薦: 100%|██████████| 4/4 [00:06<00:00,  1.52s/it]\n",
      "Mobile01: 業務推薦: 0it [00:00, ?it/s]\n",
      "my83: 業務推薦: 0it [00:00, ?it/s]\n",
      "finfo: 業務推薦: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共抓取 66 筆資料！已儲存為 CSV。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 多平台保險討論爬蟲系統（PTT, Mobile01, my83, finfo）\n",
    "# ---- 安裝必要套件 ----\n",
    "# pip install requests beautifulsoup4 tqdm\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import quote\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import random\n",
    "\n",
    "# ---- 共用設定 ----\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'\n",
    "}\n",
    "KEYWORDS = [\"國泰 保險\", \"國泰人壽\", \"保單\", \"保險理賠\", \"保險詐騙\", \"業務推薦\"]\n",
    "MAX_PER_KEYWORD = 300  # 每個關鍵字最多幾篇\n",
    "\n",
    "# ---- 儲存資料工具 ----\n",
    "def save_to_csv(filename, rows):\n",
    "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"來源\", \"標題\", \"連結\", \"內容\"])\n",
    "        writer.writerows(rows)\n",
    "\n",
    "# ---- PTT 爬蟲 ----\n",
    "def crawl_ptt(keyword):\n",
    "    base_url = \"https://www.ptt.cc\"\n",
    "    search_url = f\"https://www.ptt.cc/bbs/Insurance/search?q={quote(keyword)}\"\n",
    "    rows = []\n",
    "\n",
    "    res = requests.get(search_url, headers=HEADERS)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    links = soup.select(\".title a\")\n",
    "\n",
    "    for a in tqdm(links[:MAX_PER_KEYWORD], desc=f\"PTT: {keyword}\"):\n",
    "        try:\n",
    "            url = base_url + a['href']\n",
    "            title = a.text.strip()\n",
    "            post = requests.get(url, headers=HEADERS)\n",
    "            post_soup = BeautifulSoup(post.text, \"html.parser\")\n",
    "            content = post_soup.select_one(\"#main-content\").text\n",
    "            rows.append([\"PTT\", title, url, content])\n",
    "            sleep(random.uniform(0.5, 1.2))\n",
    "        except:\n",
    "            continue\n",
    "    return rows\n",
    "\n",
    "# ---- Mobile01 爬蟲 ----\n",
    "def crawl_mobile01(keyword):\n",
    "    base_url = \"https://www.mobile01.com\"\n",
    "    search_url = f\"https://www.mobile01.com/googlesearch.php?query={quote(keyword)}\"\n",
    "    rows = []\n",
    "\n",
    "    res = requests.get(search_url, headers=HEADERS)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    links = soup.select(\".c-listTableTd__title a\")\n",
    "\n",
    "    for a in tqdm(links[:MAX_PER_KEYWORD], desc=f\"Mobile01: {keyword}\"):\n",
    "        try:\n",
    "            url = base_url + a['href']\n",
    "            title = a.text.strip()\n",
    "            post = requests.get(url, headers=HEADERS)\n",
    "            post_soup = BeautifulSoup(post.text, \"html.parser\")\n",
    "            content = post_soup.get_text(\"\\n\", strip=True)\n",
    "            rows.append([\"Mobile01\", title, url, content])\n",
    "            sleep(random.uniform(0.8, 1.5))\n",
    "        except:\n",
    "            continue\n",
    "    return rows\n",
    "\n",
    "# ---- my83.com.tw 爬蟲 ----\n",
    "def crawl_my83(keyword):\n",
    "    search_url = f\"https://my83.com.tw/question/search?q={quote(keyword)}\"\n",
    "    rows = []\n",
    "\n",
    "    res = requests.get(search_url, headers=HEADERS)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    links = soup.select(\".question-item .title a\")\n",
    "\n",
    "    for a in tqdm(links[:MAX_PER_KEYWORD], desc=f\"my83: {keyword}\"):\n",
    "        try:\n",
    "            url = a['href']\n",
    "            title = a.text.strip()\n",
    "            post = requests.get(url, headers=HEADERS)\n",
    "            post_soup = BeautifulSoup(post.text, \"html.parser\")\n",
    "            content = post_soup.get_text(\"\\n\", strip=True)\n",
    "            rows.append([\"my83\", title, url, content])\n",
    "            sleep(random.uniform(0.8, 1.3))\n",
    "        except:\n",
    "            continue\n",
    "    return rows\n",
    "\n",
    "# ---- finfo.tw 爬蟲 ----\n",
    "def crawl_finfo(keyword):\n",
    "    search_url = f\"https://finfo.tw/posts?post_q%5Bkeyword%5D={quote(keyword)}&button=\"\n",
    "    rows = []\n",
    "\n",
    "    res = requests.get(search_url, headers=HEADERS)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    links = soup.select(\".card-title a\")\n",
    "\n",
    "    for a in tqdm(links[:MAX_PER_KEYWORD], desc=f\"finfo: {keyword}\"):\n",
    "        try:\n",
    "            url = a['href'] if a['href'].startswith(\"http\") else \"https://finfo.tw\" + a['href']\n",
    "            title = a.text.strip()\n",
    "            post = requests.get(url, headers=HEADERS)\n",
    "            post_soup = BeautifulSoup(post.text, \"html.parser\")\n",
    "            content = post_soup.get_text(\"\\n\", strip=True)\n",
    "            rows.append([\"finfo\", title, url, content])\n",
    "            sleep(random.uniform(0.6, 1.2))\n",
    "        except:\n",
    "            continue\n",
    "    return rows\n",
    "\n",
    "# ---- 主流程 ----\n",
    "all_data = []\n",
    "for kw in KEYWORDS:\n",
    "    all_data += crawl_ptt(kw)\n",
    "    all_data += crawl_mobile01(kw)\n",
    "    all_data += crawl_my83(kw)\n",
    "    all_data += crawl_finfo(kw)\n",
    "\n",
    "save_to_csv(\"保險社群語料.csv\", all_data)\n",
    "print(f\"共抓取 {len(all_data)} 筆資料！已儲存為 CSV。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\robby1206\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: bs4 in c:\\users\\robby1206\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.0.2)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\robby1206\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\robby1206\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\robby1206\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\robby1206\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\robby1206\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from bs4) (4.13.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\robby1206\\appdata\\roaming\\python\\python313\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\robby1206\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4->bs4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\robby1206\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4->bs4) (4.13.1)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script tqdm.exe is installed in 'c:\\Users\\Robby1206\\AppData\\Local\\Programs\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install requests bs4 tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多平台保險討論爬蟲系統（PTT, Mobile01, my83, finfo）＋留言擴充＋多版面支援\n",
    "# ---- 安裝必要套件 ----\n",
    "# pip install requests beautifulsoup4 tqdm\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import quote\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import random\n",
    "\n",
    "# ---- 共用設定 ----\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'\n",
    "}\n",
    "KEYWORDS = [\"保險\", \"國泰\", \"國泰人壽\", \"保單\", \"理賠\", \"詐騙\", \"推薦\", \"壽險\", \"產險\"]\n",
    "PTT_BOARDS = [\"Insurance\", \"WomenTalk\", \"Stock\", \"Lifeismoney\", \"Gossiping\"]\n",
    "M01_FORUMS = [\"566\", \"376\", \"315\"]  # 理財, 職場甘苦, 生活綜合\n",
    "FIN_TAGS = [\"保險\", \"保單比較\", \"醫療險\", \"業務\"]\n",
    "MAX_PER_KEYWORD = 5000  # 每個關鍵字最多幾篇\n",
    "\n",
    "# ---- 儲存資料工具 ----\n",
    "def save_to_csv(filename, rows):\n",
    "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"來源\", \"標題\", \"連結\", \"內容\"])\n",
    "        writer.writerows(rows)\n",
    "\n",
    "# ---- PTT 爬蟲 ----\n",
    "def crawl_ptt(keyword):\n",
    "    base_url = \"https://www.ptt.cc\"\n",
    "    rows = []\n",
    "    for board in PTT_BOARDS:\n",
    "        search_url = f\"https://www.ptt.cc/bbs/{board}/search?q={quote(keyword)}\"\n",
    "        try:\n",
    "            res = requests.get(search_url, headers=HEADERS)\n",
    "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "            links = soup.select(\".title a\")\n",
    "            for a in links[:MAX_PER_KEYWORD // len(PTT_BOARDS)]:\n",
    "                url = base_url + a['href']\n",
    "                title = a.text.strip()\n",
    "                post = requests.get(url, headers=HEADERS)\n",
    "                post_soup = BeautifulSoup(post.text, \"html.parser\")\n",
    "                content = post_soup.select_one(\"#main-content\").text\n",
    "                rows.append([f\"PTT-{board}\", title, url, content])\n",
    "                # 抓留言\n",
    "                pushes = post_soup.select(\".push\")\n",
    "                for p in pushes:\n",
    "                    msg = p.text.strip()\n",
    "                    rows.append([f\"PTT-{board}-回文\", title, url, msg])\n",
    "                sleep(random.uniform(0.5, 1.2))\n",
    "        except:\n",
    "            continue\n",
    "    return rows\n",
    "\n",
    "# ---- Mobile01 爬蟲 ----\n",
    "def crawl_mobile01(keyword):\n",
    "    base_url = \"https://www.mobile01.com\"\n",
    "    rows = []\n",
    "    for fid in M01_FORUMS:\n",
    "        search_url = f\"https://www.mobile01.com/forumtopic.php?c={fid}&s={quote(keyword)}\"\n",
    "        try:\n",
    "            res = requests.get(search_url, headers=HEADERS)\n",
    "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "            links = soup.select(\".c-listTableTd__title a\")\n",
    "            for a in links[:MAX_PER_KEYWORD // len(M01_FORUMS)]:\n",
    "                url = base_url + a['href']\n",
    "                title = a.text.strip()\n",
    "                post = requests.get(url, headers=HEADERS)\n",
    "                post_soup = BeautifulSoup(post.text, \"html.parser\")\n",
    "                content = post_soup.get_text(\"\\n\", strip=True)\n",
    "                rows.append([\"Mobile01\", title, url, content])\n",
    "                sleep(random.uniform(0.8, 1.5))\n",
    "        except:\n",
    "            continue\n",
    "    return rows\n",
    "\n",
    "# ---- my83.com.tw 爬蟲 ----\n",
    "def crawl_my83(keyword):\n",
    "    search_url = f\"https://my83.com.tw/question/search?q={quote(keyword)}\"\n",
    "    rows = []\n",
    "    try:\n",
    "        res = requests.get(search_url, headers=HEADERS)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        links = soup.select(\".question-item .title a\")\n",
    "        for a in links[:MAX_PER_KEYWORD]:\n",
    "            url = a['href']\n",
    "            title = a.text.strip()\n",
    "            post = requests.get(url, headers=HEADERS)\n",
    "            post_soup = BeautifulSoup(post.text, \"html.parser\")\n",
    "            content = post_soup.get_text(\"\\n\", strip=True)\n",
    "            rows.append([\"my83\", title, url, content])\n",
    "            sleep(random.uniform(0.8, 1.3))\n",
    "    except:\n",
    "        pass\n",
    "    return rows\n",
    "\n",
    "# ---- finfo.tw 爬蟲 ----\n",
    "def crawl_finfo(keyword):\n",
    "    rows = []\n",
    "    for tag in FIN_TAGS:\n",
    "        search_url = f\"https://finfo.tw/posts?post_q%5Bkeyword%5D={quote(keyword)}&tag={quote(tag)}\"\n",
    "        try:\n",
    "            res = requests.get(search_url, headers=HEADERS)\n",
    "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "            links = soup.select(\".card-title a\")\n",
    "            for a in links[:MAX_PER_KEYWORD // len(FIN_TAGS)]:\n",
    "                url = a['href'] if a['href'].startswith(\"http\") else \"https://finfo.tw\" + a['href']\n",
    "                title = a.text.strip()\n",
    "                post = requests.get(url, headers=HEADERS)\n",
    "                post_soup = BeautifulSoup(post.text, \"html.parser\")\n",
    "                content = post_soup.get_text(\"\\n\", strip=True)\n",
    "                rows.append([f\"finfo-{tag}\", title, url, content])\n",
    "                sleep(random.uniform(0.6, 1.2))\n",
    "        except:\n",
    "            continue\n",
    "    return rows\n",
    "\n",
    "# ---- 主流程 ----\n",
    "all_data = []\n",
    "for kw in KEYWORDS:\n",
    "    all_data += crawl_ptt(kw)\n",
    "    all_data += crawl_mobile01(kw)\n",
    "    all_data += crawl_my83(kw)\n",
    "    all_data += crawl_finfo(kw)\n",
    "\n",
    "save_to_csv(\"保險社群語料_擴充版.csv\", all_data)\n",
    "print(f\"共抓取 {len(all_data)} 筆資料（含留言與多版面）！已儲存為 CSV。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
