{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0badacba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement captum.attr (from versions: none)\n",
      "ERROR: No matching distribution found for captum.attr\n"
     ]
    }
   ],
   "source": [
    "!PIP install captum.attr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be9af53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 BERT 預測帳號是否為潛在詐騙者：文字語意判斷（深度學習）\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import defaultdict\n",
    "\n",
    "# 載入資料\n",
    "mobile_df = pd.read_csv(\"mobile01_處理後.csv\")\n",
    "ptt_df = pd.read_csv(\"ptt_語料_處理後.csv\")\n",
    "finfo_df = pd.read_csv(\"finfo_posts_產險_壽險_投資型.csv\")\n",
    "\n",
    "# 平台標記\n",
    "mobile_df['平台'] = 'Mobile01'\n",
    "ptt_df['平台'] = 'PTT'\n",
    "finfo_df['平台'] = 'Finfo'\n",
    "\n",
    "# 帳號欄位統一化\n",
    "for df in [mobile_df, ptt_df, finfo_df]:\n",
    "    if '留言帳號' in df.columns and '發文者帳號' in df.columns:\n",
    "        df['帳號'] = df['留言帳號'].fillna(df['發文者帳號'])\n",
    "    elif '帳號' in df.columns:\n",
    "        df['帳號'] = df['帳號']\n",
    "    elif 'author' in df.columns:\n",
    "        df['帳號'] = df['author']\n",
    "    else:\n",
    "        df['帳號'] = '未知帳號'\n",
    "\n",
    "# 文字整合\n",
    "for df in [mobile_df, ptt_df, finfo_df]:\n",
    "    text_cols = []\n",
    "    if '留言內容' in df.columns:\n",
    "        text_cols.append(df['留言內容'].fillna(''))\n",
    "    if '發文內容' in df.columns:\n",
    "        text_cols.append(df['發文內容'].fillna(''))\n",
    "    if not text_cols:\n",
    "        df['text'] = ''\n",
    "    else:\n",
    "        df['text'] = text_cols[0]\n",
    "        for col in text_cols[1:]:\n",
    "            df['text'] += ' ' + col\n",
    "\n",
    "# 詐騙關鍵詞欄位補上（如無則設 0）\n",
    "for df in [mobile_df, ptt_df, finfo_df]:\n",
    "    if '詐騙關鍵詞次數' not in df.columns:\n",
    "        df['詐騙關鍵詞次數'] = 0\n",
    "\n",
    "# 合併資料\n",
    "combined_df = pd.concat([\n",
    "    mobile_df[['平台', '帳號', 'text', '詐騙關鍵詞次數']],\n",
    "    ptt_df[['平台', '帳號', 'text', '詐騙關鍵詞次數']],\n",
    "    finfo_df[['平台', '帳號', 'text', '詐騙關鍵詞次數']]\n",
    "], ignore_index=True)\n",
    "\n",
    "# 清理與標籤\n",
    "combined_df = combined_df.dropna(subset=['帳號'])\n",
    "combined_df = combined_df[combined_df['text'].str.strip() != '']\n",
    "combined_df['label'] = (combined_df['詐騙關鍵詞次數'] > 0).astype(int)\n",
    "\n",
    "# 分割資料\n",
    "train_texts, test_texts, train_labels, test_labels, train_accounts, test_accounts = train_test_split(\n",
    "    combined_df['text'].values,\n",
    "    combined_df['label'].values,\n",
    "    combined_df['帳號'].values,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Tokenizer 初始化\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "# 自訂 Dataset\n",
    "class ScamDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=128)\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = ScamDataset(train_texts, train_labels)\n",
    "test_dataset = ScamDataset(test_texts, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# 定義模型\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-chinese')\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        dropped = self.dropout(pooled_output)\n",
    "        return torch.sigmoid(self.classifier(dropped)).squeeze()\n",
    "\n",
    "# 初始化模型與優化器\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BertClassifier().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# 訓練迴圈\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].float().to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} 完成，Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 預測與帳號風險統計\n",
    "model.eval()\n",
    "preds, trues, probs = [], [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        probs.extend(outputs.cpu().numpy())\n",
    "        pred_labels = (outputs > 0.5).int().cpu().numpy()\n",
    "        preds.extend(pred_labels)\n",
    "        trues.extend(labels.cpu().numpy())\n",
    "\n",
    "print(\"\\n分類報告：\")\n",
    "print(classification_report(trues, preds))\n",
    "\n",
    "# 聚合帳號風險分數\n",
    "account_risk = defaultdict(list)\n",
    "for acct, prob in zip(test_accounts, probs):\n",
    "    account_risk[acct].append(prob)\n",
    "\n",
    "account_avg_risk = [(acct, np.mean(scores), len(scores)) for acct, scores in account_risk.items() if len(scores) >= 2]\n",
    "account_risk_df = pd.DataFrame(account_avg_risk, columns=['帳號', '平均風險機率', '樣本數']).sort_values(by='平均風險機率', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 潛在高風險帳號：\")\n",
    "print(account_risk_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c872719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用裝置： cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ckiplab/bert-base-chinese and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 125\u001b[0m\n\u001b[0;32m    122\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels)\n\u001b[0;32m    124\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 125\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    126\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    128\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\Robby1206\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    628\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Robby1206\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Robby1206\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 使用 DistilBERT 預測帳號是否為潛在詐騙者（適用全量資料）\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import joblib\n",
    "\n",
    "\n",
    "# 檢查 GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"使用裝置：\", device)\n",
    "\n",
    "# 載入資料\n",
    "mobile_df = pd.read_csv(\"mobile01_處理後.csv\")\n",
    "ptt_df = pd.read_csv(\"ptt_語料_處理後.csv\")\n",
    "finfo_df = pd.read_csv(\"finfo_posts_產險_壽險_投資型.csv\")\n",
    "\n",
    "# 平台標記\n",
    "mobile_df['平台'] = 'Mobile01'\n",
    "ptt_df['平台'] = 'PTT'\n",
    "finfo_df['平台'] = 'Finfo'\n",
    "\n",
    "# 帳號欄位統一化\n",
    "for df in [mobile_df, ptt_df, finfo_df]:\n",
    "    if '留言帳號' in df.columns and '發文者帳號' in df.columns:\n",
    "        df['帳號'] = df['留言帳號'].fillna(df['發文者帳號'])\n",
    "    elif '帳號' in df.columns:\n",
    "        df['帳號'] = df['帳號']\n",
    "    elif 'author' in df.columns:\n",
    "        df['帳號'] = df['author']\n",
    "    else:\n",
    "        df['帳號'] = '未知帳號'\n",
    "\n",
    "    df['留言內容'] = df['留言內容'] if '留言內容' in df.columns else ''\n",
    "    df['發文內容'] = df['發文內容'] if '發文內容' in df.columns else ''\n",
    "    df['text'] = df['留言內容'].fillna('') + ' ' + df['發文內容'].fillna('')\n",
    "\n",
    "    if '詐騙關鍵詞次數' not in df.columns:\n",
    "        df['詐騙關鍵詞次數'] = 0\n",
    "\n",
    "# 合併資料\n",
    "combined_df = pd.concat([\n",
    "    mobile_df[['平台', '帳號', 'text', '詐騙關鍵詞次數']],\n",
    "    ptt_df[['平台', '帳號', 'text', '詐騙關鍵詞次數']],\n",
    "    finfo_df[['平台', '帳號', 'text', '詐騙關鍵詞次數']]\n",
    "], ignore_index=True)\n",
    "\n",
    "combined_df = combined_df.dropna(subset=['帳號'])\n",
    "combined_df = combined_df[combined_df['text'].str.strip() != '']\n",
    "combined_df['label'] = (combined_df['詐騙關鍵詞次數'] > 0).astype(int)\n",
    "\n",
    "# 分割資料\n",
    "train_texts, test_texts, train_labels, test_labels, train_accounts, test_accounts = train_test_split(\n",
    "    combined_df['text'].values,\n",
    "    combined_df['label'].values,\n",
    "    combined_df['帳號'].values,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "test_text_raw = test_texts\n",
    "\n",
    "# Tokenizer\n",
    "model_name = 'ckiplab/bert-base-chinese'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Dataset 類別\n",
    "class ScamDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=64)\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = ScamDataset(train_texts, train_labels)\n",
    "test_dataset = ScamDataset(test_texts, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4)\n",
    "\n",
    "# 模型定義\n",
    "class DistilBertClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]\n",
    "        dropped = self.dropout(pooled_output)\n",
    "        return torch.sigmoid(self.classifier(dropped)).squeeze()\n",
    "\n",
    "model = DistilBertClassifier().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# 儲存指標\n",
    "history = {'loss': [], 'accuracy': [], 'f1': []}\n",
    "\n",
    "# 訓練\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].float().to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Epoch {epoch+1} / 3，Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # 預測（每 epoch）\n",
    "    model.eval()\n",
    "    preds, trues, probs = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            probs.extend(outputs.cpu().numpy())\n",
    "            pred_labels = (outputs > 0.5).int().cpu().numpy()\n",
    "            preds.extend(pred_labels)\n",
    "            trues.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(trues, preds)\n",
    "    f1 = f1_score(trues, preds)\n",
    "    history['loss'].append(total_loss / len(train_loader))\n",
    "    history['accuracy'].append(acc)\n",
    "    history['f1'].append(f1)\n",
    "\n",
    "    print(f\"Accuracy: {acc:.4f} | F1-score: {f1:.4f}\")\n",
    "\n",
    "    # 選項 A：帳號風險統計 + 儲存\n",
    "    account_risk = defaultdict(list)\n",
    "    for acct, prob in zip(test_accounts, probs):\n",
    "        account_risk[acct].append(prob)\n",
    "\n",
    "    account_avg_risk = [(acct, np.mean(scores), len(scores)) for acct, scores in account_risk.items() if len(scores) >= 2]\n",
    "    account_risk_df = pd.DataFrame(account_avg_risk, columns=['帳號', '平均風險機率', '樣本數']).sort_values(by='平均風險機率', ascending=False)\n",
    "    account_risk_df.to_csv(f\"account_risk_epoch{epoch+1}.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "    # 選項 B：逐筆預測結果儲存\n",
    "    detailed_df = pd.DataFrame({\n",
    "        '帳號': test_accounts,\n",
    "        '原始文字': test_text_raw,\n",
    "        '真實標籤': trues,\n",
    "        '預測機率': probs,\n",
    "        '預測結果': preds\n",
    "    })\n",
    "    detailed_df.to_csv(f\"predictions_epoch{epoch+1}.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "# 儲存最終模型與 tokenizer\n",
    "joblib.dump(model.state_dict(), \"distilbert_scam_model.pt\")\n",
    "tokenizer.save_pretrained(\"distilbert_tokenizer\")\n",
    "\n",
    "# 畫 loss 與指標圖\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history['loss'], marker='o')\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history['accuracy'], marker='o')\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(history['f1'], marker='o')\n",
    "plt.title(\"F1-score\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"training_metrics.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 20 潛在高風險帳號：\")\n",
    "print(account_risk_df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b21ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安裝必要套件（第一次執行需要）\n",
    "!pip install transformers datasets scikit-learn --quiet\n",
    "\n",
    "# 載入套件\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "# === 1. 準備你的資料 ===\n",
    "# 假設你有一份 DataFrame 叫 df，有兩欄：'text' 和 'label'（0 = 正常, 1 = 詐騙）\n",
    "# 你可以用自己的資料匯入，這裡做個模擬\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "\n",
    "# 分訓練/驗證集\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "\n",
    "# 轉換成 HuggingFace Dataset 格式\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "# === 2. 下載 tokenizer & 處理文字 ===\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['text'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# === 3. 設定模型 ===\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# === 4. 訓練參數設定 ===\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,           # 適合 GTX1060\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_dir='./logs',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    ")\n",
    "\n",
    "# === 5. 計算指標 ===\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n",
    "# === 6. 初始化 Trainer 並訓練 ===\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# === 7. 開始訓練 ===\n",
    "trainer.train()\n",
    "\n",
    "# === 8. 評估驗證集表現 ===\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfe447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ✅ 1. 安裝套件 + 掛載 Google Drive\n",
    "# ========================================\n",
    "!pip install transformers datasets scikit-learn seaborn --quiet\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ========================================\n",
    "# ✅ 2. 載入資料 + 清理欄位\n",
    "# ========================================\n",
    "df1 = pd.read_csv('/content/drive/MyDrive/finfo_posts_產險_壽險_投資型.csv')\n",
    "df2 = pd.read_csv('/content/drive/MyDrive/mobile01_處理後.csv')\n",
    "df3 = pd.read_csv('/content/drive/MyDrive/ptt_語料_處理後.csv')\n",
    "\n",
    "for df in [df1, df2, df3]:\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "def extract_text(row):\n",
    "    text = ''\n",
    "    if '發文內容' in row and pd.notna(row['發文內容']):\n",
    "        text += str(row['發文內容']) + ' '\n",
    "    if '留言內容' in row and pd.notna(row['留言內容']):\n",
    "        text += str(row['留言內容'])\n",
    "    elif 'content' in row and pd.notna(row['content']):\n",
    "        text += str(row['content'])\n",
    "    return text.strip()\n",
    "\n",
    "def prepare_labeled_df(df):\n",
    "    df['text'] = df.apply(extract_text, axis=1)\n",
    "    df['label'] = df['詐騙關鍵詞次數'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    return df[['text', 'label']]\n",
    "\n",
    "df_train = pd.concat([prepare_labeled_df(df2), prepare_labeled_df(df3)], ignore_index=True)\n",
    "df_unlabeled = df1.copy()\n",
    "df_unlabeled['text'] = df_unlabeled.apply(extract_text, axis=1)\n",
    "\n",
    "# ========================================\n",
    "# ✅ 3. Tokenizer + Dataset 切分\n",
    "# ========================================\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "raw_dataset = Dataset.from_pandas(df_train)\n",
    "tokenized_dataset = raw_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "# ========================================\n",
    "# ✅ 4. 模型訓練 + EarlyStopping（乾淨寫法）\n",
    "# ========================================\n",
    "from transformers import BertForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-chinese\", num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/bert_results\",\n",
    "    eval_strategy=\"epoch\",  # ✅ 使用未來建議寫法\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=\"/content/drive/MyDrive/bert_logs\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\"  # ✅ 關閉 wandb 提示\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# ========================================\n",
    "# ✅ 5. 預測 df1 ➜ 輸出 prediction_result.csv\n",
    "# ========================================\n",
    "from transformers import TextClassificationPipeline\n",
    "import torch\n",
    "\n",
    "pipe = TextClassificationPipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_all_scores=True,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "results = pipe(df_unlabeled[\"text\"].tolist(), batch_size=16)\n",
    "pred_labels = [int(np.argmax([p[\"score\"] for p in res])) for res in results]\n",
    "pred_probs = [round(res[1][\"score\"], 4) for res in results]\n",
    "\n",
    "df_unlabeled[\"predicted_label\"] = pred_labels\n",
    "df_unlabeled[\"predicted_prob\"] = pred_probs\n",
    "\n",
    "output_path = \"/content/drive/MyDrive/prediction_result.csv\"\n",
    "df_unlabeled[[\"text\", \"predicted_label\", \"predicted_prob\"]].to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"✅ 預測完成！結果已儲存：{output_path}\")\n",
    "\n",
    "# ========================================\n",
    "# ✅ 6. 視覺化：分佈圖、機率直方圖\n",
    "# ========================================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.countplot(x='predicted_label', data=df_unlabeled)\n",
    "plt.title(\"📊 預測類別分佈（0=正常，1=詐騙）\")\n",
    "plt.xlabel(\"預測標籤\")\n",
    "plt.ylabel(\"樣本數\")\n",
    "plt.xticks([0, 1], ['正常 (0)', '詐騙 (1)'])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(df_unlabeled['predicted_prob'], bins=20, kde=True, color='orange')\n",
    "plt.title(\"📊 預測為詐騙的機率分佈\")\n",
    "plt.xlabel(\"詐騙機率\")\n",
    "plt.ylabel(\"樣本數\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5b9de6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
