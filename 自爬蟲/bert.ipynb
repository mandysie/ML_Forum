{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0badacba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement captum.attr (from versions: none)\n",
      "ERROR: No matching distribution found for captum.attr\n"
     ]
    }
   ],
   "source": [
    "!PIP install captum.attr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be9af53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨ BERT é æ¸¬å¸³è™Ÿæ˜¯å¦ç‚ºæ½›åœ¨è©é¨™è€…ï¼šæ–‡å­—èªæ„åˆ¤æ–·ï¼ˆæ·±åº¦å­¸ç¿’ï¼‰\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import defaultdict\n",
    "\n",
    "# è¼‰å…¥è³‡æ–™\n",
    "mobile_df = pd.read_csv(\"mobile01_è™•ç†å¾Œ.csv\")\n",
    "ptt_df = pd.read_csv(\"ptt_èªæ–™_è™•ç†å¾Œ.csv\")\n",
    "finfo_df = pd.read_csv(\"finfo_posts_ç”¢éšª_å£½éšª_æŠ•è³‡å‹.csv\")\n",
    "\n",
    "# å¹³å°æ¨™è¨˜\n",
    "mobile_df['å¹³å°'] = 'Mobile01'\n",
    "ptt_df['å¹³å°'] = 'PTT'\n",
    "finfo_df['å¹³å°'] = 'Finfo'\n",
    "\n",
    "# å¸³è™Ÿæ¬„ä½çµ±ä¸€åŒ–\n",
    "for df in [mobile_df, ptt_df, finfo_df]:\n",
    "    if 'ç•™è¨€å¸³è™Ÿ' in df.columns and 'ç™¼æ–‡è€…å¸³è™Ÿ' in df.columns:\n",
    "        df['å¸³è™Ÿ'] = df['ç•™è¨€å¸³è™Ÿ'].fillna(df['ç™¼æ–‡è€…å¸³è™Ÿ'])\n",
    "    elif 'å¸³è™Ÿ' in df.columns:\n",
    "        df['å¸³è™Ÿ'] = df['å¸³è™Ÿ']\n",
    "    elif 'author' in df.columns:\n",
    "        df['å¸³è™Ÿ'] = df['author']\n",
    "    else:\n",
    "        df['å¸³è™Ÿ'] = 'æœªçŸ¥å¸³è™Ÿ'\n",
    "\n",
    "# æ–‡å­—æ•´åˆ\n",
    "for df in [mobile_df, ptt_df, finfo_df]:\n",
    "    text_cols = []\n",
    "    if 'ç•™è¨€å…§å®¹' in df.columns:\n",
    "        text_cols.append(df['ç•™è¨€å…§å®¹'].fillna(''))\n",
    "    if 'ç™¼æ–‡å…§å®¹' in df.columns:\n",
    "        text_cols.append(df['ç™¼æ–‡å…§å®¹'].fillna(''))\n",
    "    if not text_cols:\n",
    "        df['text'] = ''\n",
    "    else:\n",
    "        df['text'] = text_cols[0]\n",
    "        for col in text_cols[1:]:\n",
    "            df['text'] += ' ' + col\n",
    "\n",
    "# è©é¨™é—œéµè©æ¬„ä½è£œä¸Šï¼ˆå¦‚ç„¡å‰‡è¨­ 0ï¼‰\n",
    "for df in [mobile_df, ptt_df, finfo_df]:\n",
    "    if 'è©é¨™é—œéµè©æ¬¡æ•¸' not in df.columns:\n",
    "        df['è©é¨™é—œéµè©æ¬¡æ•¸'] = 0\n",
    "\n",
    "# åˆä½µè³‡æ–™\n",
    "combined_df = pd.concat([\n",
    "    mobile_df[['å¹³å°', 'å¸³è™Ÿ', 'text', 'è©é¨™é—œéµè©æ¬¡æ•¸']],\n",
    "    ptt_df[['å¹³å°', 'å¸³è™Ÿ', 'text', 'è©é¨™é—œéµè©æ¬¡æ•¸']],\n",
    "    finfo_df[['å¹³å°', 'å¸³è™Ÿ', 'text', 'è©é¨™é—œéµè©æ¬¡æ•¸']]\n",
    "], ignore_index=True)\n",
    "\n",
    "# æ¸…ç†èˆ‡æ¨™ç±¤\n",
    "combined_df = combined_df.dropna(subset=['å¸³è™Ÿ'])\n",
    "combined_df = combined_df[combined_df['text'].str.strip() != '']\n",
    "combined_df['label'] = (combined_df['è©é¨™é—œéµè©æ¬¡æ•¸'] > 0).astype(int)\n",
    "\n",
    "# åˆ†å‰²è³‡æ–™\n",
    "train_texts, test_texts, train_labels, test_labels, train_accounts, test_accounts = train_test_split(\n",
    "    combined_df['text'].values,\n",
    "    combined_df['label'].values,\n",
    "    combined_df['å¸³è™Ÿ'].values,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Tokenizer åˆå§‹åŒ–\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "# è‡ªè¨‚ Dataset\n",
    "class ScamDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=128)\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = ScamDataset(train_texts, train_labels)\n",
    "test_dataset = ScamDataset(test_texts, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# å®šç¾©æ¨¡å‹\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-chinese')\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        dropped = self.dropout(pooled_output)\n",
    "        return torch.sigmoid(self.classifier(dropped)).squeeze()\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹èˆ‡å„ªåŒ–å™¨\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BertClassifier().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# è¨“ç·´è¿´åœˆ\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].float().to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} å®Œæˆï¼ŒLoss: {loss.item():.4f}\")\n",
    "\n",
    "# é æ¸¬èˆ‡å¸³è™Ÿé¢¨éšªçµ±è¨ˆ\n",
    "model.eval()\n",
    "preds, trues, probs = [], [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        probs.extend(outputs.cpu().numpy())\n",
    "        pred_labels = (outputs > 0.5).int().cpu().numpy()\n",
    "        preds.extend(pred_labels)\n",
    "        trues.extend(labels.cpu().numpy())\n",
    "\n",
    "print(\"\\nåˆ†é¡å ±å‘Šï¼š\")\n",
    "print(classification_report(trues, preds))\n",
    "\n",
    "# èšåˆå¸³è™Ÿé¢¨éšªåˆ†æ•¸\n",
    "account_risk = defaultdict(list)\n",
    "for acct, prob in zip(test_accounts, probs):\n",
    "    account_risk[acct].append(prob)\n",
    "\n",
    "account_avg_risk = [(acct, np.mean(scores), len(scores)) for acct, scores in account_risk.items() if len(scores) >= 2]\n",
    "account_risk_df = pd.DataFrame(account_avg_risk, columns=['å¸³è™Ÿ', 'å¹³å‡é¢¨éšªæ©Ÿç‡', 'æ¨£æœ¬æ•¸']).sort_values(by='å¹³å‡é¢¨éšªæ©Ÿç‡', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 æ½›åœ¨é«˜é¢¨éšªå¸³è™Ÿï¼š\")\n",
    "print(account_risk_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c872719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½¿ç”¨è£ç½®ï¼š cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ckiplab/bert-base-chinese and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 125\u001b[0m\n\u001b[0;32m    122\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels)\n\u001b[0;32m    124\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 125\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    126\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    128\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\Robby1206\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    628\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Robby1206\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Robby1206\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨ DistilBERT é æ¸¬å¸³è™Ÿæ˜¯å¦ç‚ºæ½›åœ¨è©é¨™è€…ï¼ˆé©ç”¨å…¨é‡è³‡æ–™ï¼‰\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import joblib\n",
    "\n",
    "\n",
    "# æª¢æŸ¥ GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"ä½¿ç”¨è£ç½®ï¼š\", device)\n",
    "\n",
    "# è¼‰å…¥è³‡æ–™\n",
    "mobile_df = pd.read_csv(\"mobile01_è™•ç†å¾Œ.csv\")\n",
    "ptt_df = pd.read_csv(\"ptt_èªæ–™_è™•ç†å¾Œ.csv\")\n",
    "finfo_df = pd.read_csv(\"finfo_posts_ç”¢éšª_å£½éšª_æŠ•è³‡å‹.csv\")\n",
    "\n",
    "# å¹³å°æ¨™è¨˜\n",
    "mobile_df['å¹³å°'] = 'Mobile01'\n",
    "ptt_df['å¹³å°'] = 'PTT'\n",
    "finfo_df['å¹³å°'] = 'Finfo'\n",
    "\n",
    "# å¸³è™Ÿæ¬„ä½çµ±ä¸€åŒ–\n",
    "for df in [mobile_df, ptt_df, finfo_df]:\n",
    "    if 'ç•™è¨€å¸³è™Ÿ' in df.columns and 'ç™¼æ–‡è€…å¸³è™Ÿ' in df.columns:\n",
    "        df['å¸³è™Ÿ'] = df['ç•™è¨€å¸³è™Ÿ'].fillna(df['ç™¼æ–‡è€…å¸³è™Ÿ'])\n",
    "    elif 'å¸³è™Ÿ' in df.columns:\n",
    "        df['å¸³è™Ÿ'] = df['å¸³è™Ÿ']\n",
    "    elif 'author' in df.columns:\n",
    "        df['å¸³è™Ÿ'] = df['author']\n",
    "    else:\n",
    "        df['å¸³è™Ÿ'] = 'æœªçŸ¥å¸³è™Ÿ'\n",
    "\n",
    "    df['ç•™è¨€å…§å®¹'] = df['ç•™è¨€å…§å®¹'] if 'ç•™è¨€å…§å®¹' in df.columns else ''\n",
    "    df['ç™¼æ–‡å…§å®¹'] = df['ç™¼æ–‡å…§å®¹'] if 'ç™¼æ–‡å…§å®¹' in df.columns else ''\n",
    "    df['text'] = df['ç•™è¨€å…§å®¹'].fillna('') + ' ' + df['ç™¼æ–‡å…§å®¹'].fillna('')\n",
    "\n",
    "    if 'è©é¨™é—œéµè©æ¬¡æ•¸' not in df.columns:\n",
    "        df['è©é¨™é—œéµè©æ¬¡æ•¸'] = 0\n",
    "\n",
    "# åˆä½µè³‡æ–™\n",
    "combined_df = pd.concat([\n",
    "    mobile_df[['å¹³å°', 'å¸³è™Ÿ', 'text', 'è©é¨™é—œéµè©æ¬¡æ•¸']],\n",
    "    ptt_df[['å¹³å°', 'å¸³è™Ÿ', 'text', 'è©é¨™é—œéµè©æ¬¡æ•¸']],\n",
    "    finfo_df[['å¹³å°', 'å¸³è™Ÿ', 'text', 'è©é¨™é—œéµè©æ¬¡æ•¸']]\n",
    "], ignore_index=True)\n",
    "\n",
    "combined_df = combined_df.dropna(subset=['å¸³è™Ÿ'])\n",
    "combined_df = combined_df[combined_df['text'].str.strip() != '']\n",
    "combined_df['label'] = (combined_df['è©é¨™é—œéµè©æ¬¡æ•¸'] > 0).astype(int)\n",
    "\n",
    "# åˆ†å‰²è³‡æ–™\n",
    "train_texts, test_texts, train_labels, test_labels, train_accounts, test_accounts = train_test_split(\n",
    "    combined_df['text'].values,\n",
    "    combined_df['label'].values,\n",
    "    combined_df['å¸³è™Ÿ'].values,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "test_text_raw = test_texts\n",
    "\n",
    "# Tokenizer\n",
    "model_name = 'ckiplab/bert-base-chinese'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Dataset é¡åˆ¥\n",
    "class ScamDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=64)\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = ScamDataset(train_texts, train_labels)\n",
    "test_dataset = ScamDataset(test_texts, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4)\n",
    "\n",
    "# æ¨¡å‹å®šç¾©\n",
    "class DistilBertClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]\n",
    "        dropped = self.dropout(pooled_output)\n",
    "        return torch.sigmoid(self.classifier(dropped)).squeeze()\n",
    "\n",
    "model = DistilBertClassifier().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# å„²å­˜æŒ‡æ¨™\n",
    "history = {'loss': [], 'accuracy': [], 'f1': []}\n",
    "\n",
    "# è¨“ç·´\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].float().to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Epoch {epoch+1} / 3ï¼ŒLoss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # é æ¸¬ï¼ˆæ¯ epochï¼‰\n",
    "    model.eval()\n",
    "    preds, trues, probs = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            probs.extend(outputs.cpu().numpy())\n",
    "            pred_labels = (outputs > 0.5).int().cpu().numpy()\n",
    "            preds.extend(pred_labels)\n",
    "            trues.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(trues, preds)\n",
    "    f1 = f1_score(trues, preds)\n",
    "    history['loss'].append(total_loss / len(train_loader))\n",
    "    history['accuracy'].append(acc)\n",
    "    history['f1'].append(f1)\n",
    "\n",
    "    print(f\"Accuracy: {acc:.4f} | F1-score: {f1:.4f}\")\n",
    "\n",
    "    # é¸é … Aï¼šå¸³è™Ÿé¢¨éšªçµ±è¨ˆ + å„²å­˜\n",
    "    account_risk = defaultdict(list)\n",
    "    for acct, prob in zip(test_accounts, probs):\n",
    "        account_risk[acct].append(prob)\n",
    "\n",
    "    account_avg_risk = [(acct, np.mean(scores), len(scores)) for acct, scores in account_risk.items() if len(scores) >= 2]\n",
    "    account_risk_df = pd.DataFrame(account_avg_risk, columns=['å¸³è™Ÿ', 'å¹³å‡é¢¨éšªæ©Ÿç‡', 'æ¨£æœ¬æ•¸']).sort_values(by='å¹³å‡é¢¨éšªæ©Ÿç‡', ascending=False)\n",
    "    account_risk_df.to_csv(f\"account_risk_epoch{epoch+1}.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "    # é¸é … Bï¼šé€ç­†é æ¸¬çµæœå„²å­˜\n",
    "    detailed_df = pd.DataFrame({\n",
    "        'å¸³è™Ÿ': test_accounts,\n",
    "        'åŸå§‹æ–‡å­—': test_text_raw,\n",
    "        'çœŸå¯¦æ¨™ç±¤': trues,\n",
    "        'é æ¸¬æ©Ÿç‡': probs,\n",
    "        'é æ¸¬çµæœ': preds\n",
    "    })\n",
    "    detailed_df.to_csv(f\"predictions_epoch{epoch+1}.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "# å„²å­˜æœ€çµ‚æ¨¡å‹èˆ‡ tokenizer\n",
    "joblib.dump(model.state_dict(), \"distilbert_scam_model.pt\")\n",
    "tokenizer.save_pretrained(\"distilbert_tokenizer\")\n",
    "\n",
    "# ç•« loss èˆ‡æŒ‡æ¨™åœ–\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history['loss'], marker='o')\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history['accuracy'], marker='o')\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(history['f1'], marker='o')\n",
    "plt.title(\"F1-score\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"training_metrics.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 20 æ½›åœ¨é«˜é¢¨éšªå¸³è™Ÿï¼š\")\n",
    "print(account_risk_df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b21ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£å¿…è¦å¥—ä»¶ï¼ˆç¬¬ä¸€æ¬¡åŸ·è¡Œéœ€è¦ï¼‰\n",
    "!pip install transformers datasets scikit-learn --quiet\n",
    "\n",
    "# è¼‰å…¥å¥—ä»¶\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "# === 1. æº–å‚™ä½ çš„è³‡æ–™ ===\n",
    "# å‡è¨­ä½ æœ‰ä¸€ä»½ DataFrame å« dfï¼Œæœ‰å…©æ¬„ï¼š'text' å’Œ 'label'ï¼ˆ0 = æ­£å¸¸, 1 = è©é¨™ï¼‰\n",
    "# ä½ å¯ä»¥ç”¨è‡ªå·±çš„è³‡æ–™åŒ¯å…¥ï¼Œé€™è£¡åšå€‹æ¨¡æ“¬\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "\n",
    "# åˆ†è¨“ç·´/é©—è­‰é›†\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "\n",
    "# è½‰æ›æˆ HuggingFace Dataset æ ¼å¼\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "# === 2. ä¸‹è¼‰ tokenizer & è™•ç†æ–‡å­— ===\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['text'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# === 3. è¨­å®šæ¨¡å‹ ===\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# === 4. è¨“ç·´åƒæ•¸è¨­å®š ===\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,           # é©åˆ GTX1060\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_dir='./logs',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    ")\n",
    "\n",
    "# === 5. è¨ˆç®—æŒ‡æ¨™ ===\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n",
    "# === 6. åˆå§‹åŒ– Trainer ä¸¦è¨“ç·´ ===\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# === 7. é–‹å§‹è¨“ç·´ ===\n",
    "trainer.train()\n",
    "\n",
    "# === 8. è©•ä¼°é©—è­‰é›†è¡¨ç¾ ===\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfe447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# âœ… 1. å®‰è£å¥—ä»¶ + æ›è¼‰ Google Drive\n",
    "# ========================================\n",
    "!pip install transformers datasets scikit-learn seaborn --quiet\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ========================================\n",
    "# âœ… 2. è¼‰å…¥è³‡æ–™ + æ¸…ç†æ¬„ä½\n",
    "# ========================================\n",
    "df1 = pd.read_csv('/content/drive/MyDrive/finfo_posts_ç”¢éšª_å£½éšª_æŠ•è³‡å‹.csv')\n",
    "df2 = pd.read_csv('/content/drive/MyDrive/mobile01_è™•ç†å¾Œ.csv')\n",
    "df3 = pd.read_csv('/content/drive/MyDrive/ptt_èªæ–™_è™•ç†å¾Œ.csv')\n",
    "\n",
    "for df in [df1, df2, df3]:\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "def extract_text(row):\n",
    "    text = ''\n",
    "    if 'ç™¼æ–‡å…§å®¹' in row and pd.notna(row['ç™¼æ–‡å…§å®¹']):\n",
    "        text += str(row['ç™¼æ–‡å…§å®¹']) + ' '\n",
    "    if 'ç•™è¨€å…§å®¹' in row and pd.notna(row['ç•™è¨€å…§å®¹']):\n",
    "        text += str(row['ç•™è¨€å…§å®¹'])\n",
    "    elif 'content' in row and pd.notna(row['content']):\n",
    "        text += str(row['content'])\n",
    "    return text.strip()\n",
    "\n",
    "def prepare_labeled_df(df):\n",
    "    df['text'] = df.apply(extract_text, axis=1)\n",
    "    df['label'] = df['è©é¨™é—œéµè©æ¬¡æ•¸'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    return df[['text', 'label']]\n",
    "\n",
    "df_train = pd.concat([prepare_labeled_df(df2), prepare_labeled_df(df3)], ignore_index=True)\n",
    "df_unlabeled = df1.copy()\n",
    "df_unlabeled['text'] = df_unlabeled.apply(extract_text, axis=1)\n",
    "\n",
    "# ========================================\n",
    "# âœ… 3. Tokenizer + Dataset åˆ‡åˆ†\n",
    "# ========================================\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "raw_dataset = Dataset.from_pandas(df_train)\n",
    "tokenized_dataset = raw_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "# ========================================\n",
    "# âœ… 4. æ¨¡å‹è¨“ç·´ + EarlyStoppingï¼ˆä¹¾æ·¨å¯«æ³•ï¼‰\n",
    "# ========================================\n",
    "from transformers import BertForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-chinese\", num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/bert_results\",\n",
    "    eval_strategy=\"epoch\",  # âœ… ä½¿ç”¨æœªä¾†å»ºè­°å¯«æ³•\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=\"/content/drive/MyDrive/bert_logs\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\"  # âœ… é—œé–‰ wandb æç¤º\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# ========================================\n",
    "# âœ… 5. é æ¸¬ df1 âœ è¼¸å‡º prediction_result.csv\n",
    "# ========================================\n",
    "from transformers import TextClassificationPipeline\n",
    "import torch\n",
    "\n",
    "pipe = TextClassificationPipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_all_scores=True,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "results = pipe(df_unlabeled[\"text\"].tolist(), batch_size=16)\n",
    "pred_labels = [int(np.argmax([p[\"score\"] for p in res])) for res in results]\n",
    "pred_probs = [round(res[1][\"score\"], 4) for res in results]\n",
    "\n",
    "df_unlabeled[\"predicted_label\"] = pred_labels\n",
    "df_unlabeled[\"predicted_prob\"] = pred_probs\n",
    "\n",
    "output_path = \"/content/drive/MyDrive/prediction_result.csv\"\n",
    "df_unlabeled[[\"text\", \"predicted_label\", \"predicted_prob\"]].to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"âœ… é æ¸¬å®Œæˆï¼çµæœå·²å„²å­˜ï¼š{output_path}\")\n",
    "\n",
    "# ========================================\n",
    "# âœ… 6. è¦–è¦ºåŒ–ï¼šåˆ†ä½ˆåœ–ã€æ©Ÿç‡ç›´æ–¹åœ–\n",
    "# ========================================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.countplot(x='predicted_label', data=df_unlabeled)\n",
    "plt.title(\"ğŸ“Š é æ¸¬é¡åˆ¥åˆ†ä½ˆï¼ˆ0=æ­£å¸¸ï¼Œ1=è©é¨™ï¼‰\")\n",
    "plt.xlabel(\"é æ¸¬æ¨™ç±¤\")\n",
    "plt.ylabel(\"æ¨£æœ¬æ•¸\")\n",
    "plt.xticks([0, 1], ['æ­£å¸¸ (0)', 'è©é¨™ (1)'])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(df_unlabeled['predicted_prob'], bins=20, kde=True, color='orange')\n",
    "plt.title(\"ğŸ“Š é æ¸¬ç‚ºè©é¨™çš„æ©Ÿç‡åˆ†ä½ˆ\")\n",
    "plt.xlabel(\"è©é¨™æ©Ÿç‡\")\n",
    "plt.ylabel(\"æ¨£æœ¬æ•¸\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5b9de6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
