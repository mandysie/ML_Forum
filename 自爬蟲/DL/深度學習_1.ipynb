{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb718bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===============================\n",
    "# # ✅ 必要套件安裝 (請在終端機執行)\n",
    "# # ===============================\n",
    "# # pip install transformers datasets scikit-learn pandas torch torchvision torchaudio matplotlib seaborn openpyxl\n",
    "\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from datasets import Dataset\n",
    "# from transformers import (\n",
    "#     BertTokenizerFast,\n",
    "#     BertForSequenceClassification,\n",
    "#     TrainingArguments,\n",
    "#     Trainer,\n",
    "#     EarlyStoppingCallback,\n",
    "#     TextClassificationPipeline\n",
    "# )\n",
    "# from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "# import matplotlib.font_manager as fm # For font checking/setting\n",
    "\n",
    "# # ===============================\n",
    "# # ✅ 路徑設定 (請根據你的本機資料夾結構修改)\n",
    "# # ===============================\n",
    "# DATA_DIR = \"./data/\" # 存放 CSV 檔案的資料夾\n",
    "# OUTPUT_DIR = \"./output/\" # 存放模型、結果、圖片的資料夾\n",
    "\n",
    "# # 確保輸出資料夾存在\n",
    "# os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "# os.makedirs(os.path.join(OUTPUT_DIR, \"bert_results\"), exist_ok=True)\n",
    "# os.makedirs(os.path.join(OUTPUT_DIR, \"bert_logs\"), exist_ok=True)\n",
    "# os.makedirs(os.path.join(OUTPUT_DIR, \"bert_model_chinese\"), exist_ok=True)\n",
    "\n",
    "# # CSV 檔案路徑\n",
    "# FILE_DF1 = os.path.join(DATA_DIR, 'finfo_posts_產險_壽險_投資型.csv')\n",
    "# FILE_DF2 = os.path.join(DATA_DIR, 'mobile01_處理後.csv')\n",
    "# FILE_DF3 = os.path.join(DATA_DIR, 'ptt_語料_處理後.csv')\n",
    "\n",
    "# # 輸出檔案路徑\n",
    "# MODEL_OUTPUT_DIR = os.path.join(OUTPUT_DIR, \"bert_results\")\n",
    "# LOGGING_DIR = os.path.join(OUTPUT_DIR, \"bert_logs\")\n",
    "# SAVED_MODEL_DIR = os.path.join(OUTPUT_DIR, \"bert_model_chinese\")\n",
    "# PREDICTION_CSV_PATH = os.path.join(OUTPUT_DIR, \"prediction_result.csv\")\n",
    "# PLOT_PATH = os.path.join(OUTPUT_DIR, \"prediction_probability_distribution.png\")\n",
    "\n",
    "\n",
    "# # ===============================\n",
    "# # ✅ 設定：快速訓練模式開關\n",
    "# # ===============================\n",
    "# QUICK_MODE = True  # True ➜ 抽樣快速訓練，False ➜ 全資料\n",
    "\n",
    "# # ===============================\n",
    "# # ✅ 載入資料（含欄位清理）\n",
    "# # ===============================\n",
    "# print(\"🔄 開始載入與處理資料...\")\n",
    "# try:\n",
    "#     df1 = pd.read_csv(\"DATA_DIR/finfo_posts_產險_壽險_投資型.csv\")\n",
    "#     df2 = pd.read_csv(\"DATA_DIR/mobile01_處理後.csv\")\n",
    "#     df3 = pd.read_csv(\"DATA_DIR/ptt_語料_處理後.csv\")\n",
    "# except FileNotFoundError as e:\n",
    "#     print(f\"❌ 錯誤：找不到資料檔案！請確認路徑 '{DATA_DIR}' 下有指定的 CSV 檔案。\")\n",
    "#     print(f\"詳細錯誤：{e}\")\n",
    "#     exit() # 找不到檔案直接結束程式\n",
    "\n",
    "# for df in [df1, df2, df3]:\n",
    "#     df.columns = df.columns.str.strip()\n",
    "\n",
    "# def extract_text(row):\n",
    "#     text = ''\n",
    "#     # 優先使用 '發文內容'/'留言內容' 或 'content'\n",
    "#     if 'content' in row and pd.notna(row['content']):\n",
    "#          text += str(row['content']) + ' '\n",
    "#     elif '發文內容' in row and pd.notna(row['發文內容']):\n",
    "#         text += str(row['發文內容']) + ' '\n",
    "#         # Finfos 可能同時有發文和留言，都加進去\n",
    "#         if '留言內容' in row and pd.notna(row['留言內容']):\n",
    "#            text += str(row['留言內容'])\n",
    "\n",
    "#     # 如果上面都沒有，嘗試其他可能的欄位（根據實際資料調整）\n",
    "#     # elif '其他欄位' in row and pd.notna(row['其他欄位']):\n",
    "#     #     text += str(row['其他欄位'])\n",
    "\n",
    "#     return text.strip()\n",
    "\n",
    "# def prepare_labeled_df(df):\n",
    "#     df['text'] = df.apply(extract_text, axis=1)\n",
    "#     # 確保 '詐騙關鍵詞次數' 欄位存在且為數值，若不存在或非數值則預設為0\n",
    "#     if '詐騙關鍵詞次數' not in df.columns:\n",
    "#         print(f\"⚠️ 警告：資料集中缺少 '詐騙關鍵詞次數' 欄位，將假設所有 label 為 0。\")\n",
    "#         df['label'] = 0\n",
    "#     else:\n",
    "#         # 嘗試轉換為數值，無法轉換的設為 0\n",
    "#         df['詐騙關鍵詞次數'] = pd.to_numeric(df['詐騙關鍵詞次數'], errors='coerce').fillna(0)\n",
    "#         df['label'] = df['詐騙關鍵詞次數'].apply(lambda x: 1 if x > 0 else 0)\n",
    "#     return df[['text', 'label']]\n",
    "\n",
    "# # 處理 df2 和 df3 作為訓練資料\n",
    "# df_train_list = []\n",
    "# print(\"處理 df2 (Mobile01)...\")\n",
    "# df_train_list.append(prepare_labeled_df(df2))\n",
    "# print(\"處理 df3 (PTT)...\")\n",
    "# df_train_list.append(prepare_labeled_df(df3))\n",
    "# df_train = pd.concat(df_train_list, ignore_index=True)\n",
    "\n",
    "# # 處理 df1 作為預測資料\n",
    "# print(\"處理 df1 (Finfo)...\")\n",
    "# df_unlabeled = df1.copy()\n",
    "# df_unlabeled['text'] = df_unlabeled.apply(extract_text, axis=1)\n",
    "\n",
    "# # 移除 text 為空字串的資料 (避免 Tokenizer 出錯)\n",
    "# df_train = df_train[df_train['text'].str.strip() != '']\n",
    "# df_unlabeled = df_unlabeled[df_unlabeled['text'].str.strip() != '']\n",
    "\n",
    "# # 🔹 快速模式：抽樣訓練資料\n",
    "# if QUICK_MODE:\n",
    "#     n_samples = min(10000, len(df_train)) # 防止總資料少於一萬筆而出錯\n",
    "#     if len(df_train) > n_samples:\n",
    "#         df_train = df_train.sample(n=n_samples, random_state=42)\n",
    "#         print(f\"⚡ 快速模式啟用：抽樣 {n_samples} 筆資料訓練\")\n",
    "#     else:\n",
    "#         print(f\"⚡ 快速模式啟用：使用全部 {len(df_train)} 筆資料訓練 (總數少於或等於抽樣數)\")\n",
    "\n",
    "\n",
    "# print(f\"✅ 清理與準備完成：\")\n",
    "# print(f\"   訓練樣本數：{len(df_train)}\")\n",
    "# print(f\"   預測樣本數：{len(df_unlabeled)}\")\n",
    "# print(f\"   訓練資料標籤分佈:\\n{df_train['label'].value_counts(normalize=True)}\")\n",
    "\n",
    "\n",
    "# # ===============================\n",
    "# # ✅ tokenizer + 切分 Dataset\n",
    "# # ===============================\n",
    "# print(\"\\n🔄 開始 Tokenization 與資料集準備...\")\n",
    "# # ... (tokenizer 載入, tokenize_function 定義, Dataset.from_pandas, map 等不變) ...\n",
    "\n",
    "# try:\n",
    "#     raw_dataset = Dataset.from_pandas(df_train)\n",
    "#     # 移除原始 text 節省記憶體, 確保 'label' 欄位存在以便後續轉換\n",
    "#     tokenized_dataset = raw_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
    "#     tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\") # 先改名\n",
    "#     tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "#     # --- 👇👇👇 新增：轉換 labels 欄位為 ClassLabel 👇👇👇 ---\n",
    "#     from datasets import ClassLabel, Features # 確保導入\n",
    "#     print(\"🔄 Casting 'labels' column to ClassLabel for stratification...\")\n",
    "#     # 定義新的 Features，明確指定 'labels' 的類型\n",
    "#     new_features = tokenized_dataset.features.copy()\n",
    "#     new_features['labels'] = ClassLabel(num_classes=2, names=['not_scam', 'scam']) # num_classes=2 因為是二元分類\n",
    "#     tokenized_dataset = tokenized_dataset.cast(new_features)\n",
    "#     print(f\"✅ 'labels' column type after casting: {tokenized_dataset.features['labels']}\")\n",
    "#     # --- 👆👆👆 新增結束 👆👆👆 ---\n",
    "\n",
    "\n",
    "#     # 現在可以進行分層抽樣了\n",
    "#     split_dataset = tokenized_dataset.train_test_split(\n",
    "#         test_size=0.2,\n",
    "#         seed=42,\n",
    "#         stratify_by_column=\"labels\" # 現在 labels 是 ClassLabel，可以分層了\n",
    "#     )\n",
    "#     train_dataset = split_dataset[\"train\"]\n",
    "#     eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "#     print(f\"✅ Tokenization 與資料集切分完成：\")\n",
    "#     print(f\"   Train size: {len(train_dataset)}\")\n",
    "#     print(f\"   Eval size: {len(eval_dataset)}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"❌ Tokenization 或資料集轉換/切分失敗：{e}\")\n",
    "#     # 打印更詳細的追蹤信息，有助於除錯\n",
    "#     import traceback\n",
    "#     traceback.print_exc()\n",
    "#     exit()\n",
    "# # ===============================\n",
    "# # ✅ 模型 + 訓練設定 + EarlyStopping\n",
    "# # ===============================\n",
    "# print(\"\\n🔄 開始設定模型與訓練參數...\")\n",
    "# model = BertForSequenceClassification.from_pretrained(\"bert-base-chinese\", num_labels=2)\n",
    "\n",
    "# # --- 訓練參數 ---\n",
    "# # !! 注意 !! 如果遇到 CUDA Out of Memory (OOM) 錯誤，請優先降低 per_device_train_batch_size\n",
    "# # GTX 1060 6GB 在 max_length=256 時，batch_size 可能需要設為 4 或 2\n",
    "# # 如果 max_length=128，batch_size=8 或許可以，但仍需嘗試\n",
    "# TRAIN_BATCH_SIZE = 4 # 降低 batch size 以適應 GTX 1060 (可嘗試 8, 4, 2)\n",
    "# EVAL_BATCH_SIZE = 8 # 評估時 batch size 通常可以稍大 (可嘗試 16, 8)\n",
    "# NUM_EPOCHS = 3 if QUICK_MODE else 10\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=MODEL_OUTPUT_DIR,\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "#     per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "#     num_train_epochs=NUM_EPOCHS,\n",
    "#     weight_decay=0.01,\n",
    "#     load_best_model_at_end=True,\n",
    "#     logging_dir=LOGGING_DIR,\n",
    "#     logging_strategy=\"epoch\",\n",
    "#     metric_for_best_model=\"eval_f1\", # 改用 F1 作為主要評估指標可能更適合不平衡資料\n",
    "#     save_total_limit=1,          # 只保留最好的模型\n",
    "#     report_to=\"none\",            # 關掉 wandb 等外部報告\n",
    "#     fp16=torch.cuda.is_available(), # ✅ 自動啟用混合精度 (如果 CUDA 可用)\n",
    "#     gradient_accumulation_steps=2, # 梯度累積，等效 batch size = TRAIN_BATCH_SIZE * 2，可稍微彌補小 batch size\n",
    "#     learning_rate=3e-5,          # 常用的學習率\n",
    "# )\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     preds = np.argmax(logits, axis=-1)\n",
    "#     precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "#     acc = accuracy_score(labels, preds)\n",
    "#     return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=eval_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     callbacks=[EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.001)] # 加入 threshold 避免微小改善也繼續等\n",
    "# )\n",
    "\n",
    "# print(f\"✅ 設定完成，準備開始訓練...\")\n",
    "# print(f\"   使用 GPU: {torch.cuda.is_available()}\")\n",
    "# if torch.cuda.is_available():\n",
    "#     print(f\"   CUDA 裝置: {torch.cuda.get_device_name(0)}\")\n",
    "# print(f\"   訓練 Epochs: {NUM_EPOCHS}\")\n",
    "# print(f\"   訓練 Batch Size (Per Device): {TRAIN_BATCH_SIZE}\")\n",
    "# print(f\"   梯度累積步數: {training_args.gradient_accumulation_steps}\")\n",
    "# print(f\"   等效 Batch Size: {TRAIN_BATCH_SIZE * training_args.gradient_accumulation_steps}\")\n",
    "# print(f\"   評估 Batch Size (Per Device): {EVAL_BATCH_SIZE}\")\n",
    "# print(f\"   混合精度 (FP16): {training_args.fp16}\")\n",
    "\n",
    "# # ===============================\n",
    "# # ✅ 模型訓練\n",
    "# # ===============================\n",
    "# print(\"\\n🏋️‍♂️ 開始訓練模型...\")\n",
    "# try:\n",
    "#     trainer.train()\n",
    "#     print(\"✅ 訓練完成！\")\n",
    "# except RuntimeError as e:\n",
    "#     if \"CUDA out of memory\" in str(e):\n",
    "#         print(\"\\n❌ 錯誤：CUDA Out of Memory！\")\n",
    "#         print(\"🧠 建議：請嘗試降低 TrainingArguments 中的 'per_device_train_batch_size'。\")\n",
    "#         print(f\"   目前的設定是 {TRAIN_BATCH_SIZE}，可以試試 4 或 2。\")\n",
    "#         print(f\"   或者減少 tokenize_function 中的 'max_length' (目前是 {tokenizer.model_max_length})。\")\n",
    "#     else:\n",
    "#         print(f\"\\n❌ 訓練過程中發生錯誤：{e}\")\n",
    "#     exit()\n",
    "# except Exception as e:\n",
    "#     print(f\"\\n❌ 訓練過程中發生未預期錯誤：{e}\")\n",
    "#     exit()\n",
    "\n",
    "\n",
    "# # ===============================\n",
    "# # ✅ 儲存模型\n",
    "# # ===============================\n",
    "# print(f\"\\n💾 正在儲存最佳模型至：{SAVED_MODEL_DIR}\")\n",
    "# trainer.save_model(SAVED_MODEL_DIR)\n",
    "# tokenizer.save_pretrained(SAVED_MODEL_DIR)\n",
    "# print(\"✅ 模型與 Tokenizer 儲存完成。\")\n",
    "\n",
    "# # ===============================\n",
    "# # ✅ 預測流程 + 輸出 CSV\n",
    "# # ===============================\n",
    "# print(\"\\n📊 開始使用訓練好的模型進行預測...\")\n",
    "\n",
    "# # 重新載入最佳模型以確保使用的是儲存的版本\n",
    "# model = BertForSequenceClassification.from_pretrained(SAVED_MODEL_DIR)\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(SAVED_MODEL_DIR)\n",
    "\n",
    "# # 設定 pipeline device\n",
    "# device_num = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# pipe = TextClassificationPipeline(\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     return_all_scores=True, # 獲取所有類別分數\n",
    "#     device=device_num       # 使用 GPU (如果可用)\n",
    "# )\n",
    "\n",
    "# # 執行預測 (如果資料量大，這一步可能也需要一些時間)\n",
    "# # 使用 tolist() 避免傳遞 Series 物件可能造成的問題\n",
    "# texts_to_predict = df_unlabeled[\"text\"].tolist()\n",
    "\n",
    "# # 分批預測以控制記憶體使用\n",
    "# batch_size = 32 # 可以根據你的 RAM 和 VRAM 調整\n",
    "# results = []\n",
    "# print(f\"   預測總筆數: {len(texts_to_predict)}，批次大小: {batch_size}\")\n",
    "# for i in range(0, len(texts_to_predict), batch_size):\n",
    "#     batch_texts = texts_to_predict[i:i + batch_size]\n",
    "#     batch_results = pipe(batch_texts)\n",
    "#     results.extend(batch_results)\n",
    "#     print(f\"   已預測 {min(i + batch_size, len(texts_to_predict))}/{len(texts_to_predict)} 筆\")\n",
    "\n",
    "\n",
    "# # 處理預測結果\n",
    "# # 假設 Label 'LABEL_1' 代表詐騙 (需根據 pipeline 實際輸出確認)\n",
    "# pred_labels = []\n",
    "# pred_probs = []\n",
    "# for res in results:\n",
    "#     score_label_1 = 0.0\n",
    "#     highest_score_label = 0\n",
    "#     highest_score = 0.0\n",
    "#     for item in res:\n",
    "#         if item['label'] == 'LABEL_1':\n",
    "#             score_label_1 = item['score']\n",
    "#         if item['score'] > highest_score:\n",
    "#             highest_score = item['score']\n",
    "#             # 從 'LABEL_X' 中取出數字 X\n",
    "#             highest_score_label = int(item['label'].split('_')[-1])\n",
    "\n",
    "#     pred_labels.append(highest_score_label)\n",
    "#     pred_probs.append(round(score_label_1, 4)) # 儲存 LABEL_1 的機率\n",
    "\n",
    "# df_unlabeled[\"predicted_label\"] = pred_labels\n",
    "# df_unlabeled[\"predicted_prob_scam\"] = pred_probs # 欄位名改為更清晰\n",
    "\n",
    "# # 選擇要輸出的欄位 (只保留必要欄位，避免檔案過大)\n",
    "# output_columns = ['text', 'predicted_label', 'predicted_prob_scam']\n",
    "# # 如果原始 df1 有 ID 或其他重要欄位，也可以加入\n",
    "# # if 'id_column' in df_unlabeled.columns:\n",
    "# #     output_columns.insert(0, 'id_column')\n",
    "\n",
    "# df_unlabeled[output_columns].to_csv(PREDICTION_CSV_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# print(f\"✅ 完成預測！結果儲存於：{PREDICTION_CSV_PATH}\")\n",
    "\n",
    "\n",
    "# # ===============================\n",
    "# # ✅ (新增) 繪製預測機率分佈圖\n",
    "# # ===============================\n",
    "# print(\"\\n🎨 正在繪製預測機率分佈圖...\")\n",
    "\n",
    "# # --- 字體設定 (針對本機 Matplotlib) ---\n",
    "# # Windows: C:/Windows/Fonts/msjh.ttc (微軟正黑體)\n",
    "# # macOS: /System/Library/Fonts/STHeiti Medium.ttc (黑體-繁 中等)\n",
    "# # Linux: 可能需要先安裝字型 (如 Noto Sans CJK TC) 並找到 .ttf 或 .otf 檔\n",
    "# # 如果找不到字體或不確定，可以註解掉下面幾行，但中文可能顯示為方塊\n",
    "# try:\n",
    "#     # 嘗試設定中文字體 (請根據你的作業系統修改路徑或字體名稱)\n",
    "#     font_path = None\n",
    "#     if os.path.exists(\"C:/Windows/Fonts/msjh.ttc\"):\n",
    "#         font_path = \"C:/Windows/Fonts/msjh.ttc\"\n",
    "#         font_prop = fm.FontProperties(fname=font_path)\n",
    "#         plt.rcParams['font.family'] = font_prop.get_name()\n",
    "#     elif os.path.exists(\"/System/Library/Fonts/STHeiti Medium.ttc\"):\n",
    "#          # macOS 可能可以直接設定字體名稱\n",
    "#          plt.rcParams['font.sans-serif'] = ['Heiti TC'] # 或 'PingFang TC'\n",
    "#     elif os.path.exists(\"/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc\"): # 範例 Linux 路徑\n",
    "#         font_path = \"/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc\"\n",
    "#         font_prop = fm.FontProperties(fname=font_path)\n",
    "#         plt.rcParams['font.family'] = font_prop.get_name()\n",
    "#     else:\n",
    "#         print(\"⚠️ 未找到指定的中文字體，圖表中的中文可能無法正常顯示。\")\n",
    "\n",
    "#     plt.rcParams['axes.unicode_minus'] = False # 解決負號顯示問題\n",
    "# except Exception as font_e:\n",
    "#     print(f\"⚠️ 設定中文字體時發生錯誤: {font_e}. 中文可能無法正常顯示。\")\n",
    "# # --- 字體設定結束 ---\n",
    "\n",
    "# plt.figure(figsize=(12, 7))\n",
    "# sns.histplot(data=df_unlabeled, x=\"predicted_prob_scam\", kde=True, bins=50, color=\"skyblue\") # 使用 50 個區間\n",
    "# plt.title(\"預測為詐騙 (Label=1) 的機率分佈\", fontsize=16)\n",
    "# plt.xlabel(\"模型預測為詐騙的機率值\", fontsize=12)\n",
    "# plt.ylabel(\"資料筆數\", fontsize=12)\n",
    "# plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# # 加上閾值線\n",
    "# threshold = 0.5\n",
    "# plt.axvline(x=threshold, color='r', linestyle='--', label=f'分類閾值 = {threshold}')\n",
    "\n",
    "# # 在圖上顯示一些統計數據\n",
    "# mean_prob = df_unlabeled[\"predicted_prob_scam\"].mean()\n",
    "# median_prob = df_unlabeled[\"predicted_prob_scam\"].median()\n",
    "# plt.axvline(x=mean_prob, color='g', linestyle=':', label=f'平均機率 = {mean_prob:.3f}')\n",
    "# plt.legend(fontsize=10)\n",
    "\n",
    "# plt.tight_layout() # 自動調整邊距\n",
    "\n",
    "# # 儲存圖片\n",
    "# plt.savefig(PLOT_PATH, dpi=300) # 提高解析度\n",
    "# # plt.show() # 如果希望在 VS Code 中彈出視窗顯示圖表，取消此行註解\n",
    "\n",
    "# print(f\"✅ 機率分佈圖儲存於：{PLOT_PATH}\")\n",
    "\n",
    "\n",
    "# # ===============================\n",
    "# # ✅ (新增) 簡單解釋\n",
    "# # ===============================\n",
    "# print(\"\\n===== 📊 圖表與結果解釋 =====\")\n",
    "# print(f\"模型已對 {len(df_unlabeled)} 筆來自 '{os.path.basename(FILE_DF1)}' 的資料進行了預測。\")\n",
    "# print(f\"詳細預測結果（含原始文本、預測標籤、詐騙機率）儲存在：\\n{PREDICTION_CSV_PATH}\")\n",
    "# print(f\"\\n預測機率分佈圖儲存在：\\n{PLOT_PATH}\")\n",
    "# print(\"這張圖顯示了模型將每筆資料判斷為「詐騙」(Label=1) 的信心程度（機率值）的分佈情況。\")\n",
    "# print(\"\\n圖表觀察重點：\")\n",
    "# print(\"1. 【形狀】機率分佈是否呈現『雙峰』？\")\n",
    "# print(\"   - 若大量資料的機率集中在接近 0 和接近 1 的兩端，表示模型對大部分樣本的判斷比較『有信心』（明確判斷為非詐騙或詐騙）。\")\n",
    "# print(\"   - 若機率集中在中間（如 0.5 附近），表示模型對很多樣本的判斷比較『模糊』或『不確定』。\")\n",
    "# print(\"2. 【重心】機率主要集中在哪一邊？\")\n",
    "# print(f\"   - 平均機率 ({mean_prob:.3f}) 和中位數機率 ({median_prob:.3f}) 可以提供參考。\")\n",
    "# print(\"   - 如果大部分機率值都偏低（接近 0），表示模型傾向於認為大部分文本『不是』詐騙。\")\n",
    "# print(\"   - 如果大部分機率值都偏高（接近 1），表示模型傾向於認為大部分文本『是』詐騙。\")\n",
    "# print(\"3. 【閾值附近】紅色虛線（預設 0.5）是常用的分類界線。\")\n",
    "# print(\"   - 在這條線附近的樣本，是模型判斷結果『模棱兩可』的區域。改變閾值會影響最終哪些樣本被歸類為詐騙。\")\n",
    "# print(\"   - 這些閾值附近的樣本可能最需要人工複查。\")\n",
    "# print(\"\\n👉 建議：打開 CSV 檔案，對照高機率、低機率以及機率在閾值附近的文本，檢查模型的判斷是否符合預期。\")\n",
    "# print(\"==============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23b2101d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ROBBY1~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.797 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robby1206\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m953/953\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 52ms/step - accuracy: 0.8636 - loss: 0.3040 - val_accuracy: 0.9562 - val_loss: 0.1241\n",
      "Epoch 2/3\n",
      "\u001b[1m953/953\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 53ms/step - accuracy: 0.9636 - loss: 0.1051 - val_accuracy: 0.9585 - val_loss: 0.1232\n",
      "Epoch 3/3\n",
      "\u001b[1m953/953\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 54ms/step - accuracy: 0.9685 - loss: 0.0939 - val_accuracy: 0.9567 - val_loss: 0.1257\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 610ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9780651"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 重新載入所有必要的檔案與套件（包括 jieba）\n",
    "!pip install jieba --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# 檔案路徑\n",
    "file_paths = {\n",
    "    \"mobile01\": \"DATA_DIR/mobile01_處理後.csv\",\n",
    "    \"ptt\": \"DATA_DIR/ptt_語料_處理後.csv\",\n",
    "    \"finfo\": \"DATA_DIR/finfo_posts_產險_壽險_投資型.csv\",\n",
    "    \"keywords\": \"DATA_DIR/500精簡詐騙字詞_UTF8.csv\"\n",
    "}\n",
    "\n",
    "# 載入資料\n",
    "dfs = {name: pd.read_csv(path) for name, path in file_paths.items()}\n",
    "\n",
    "# 合併 PTT 與 Mobile01 資料\n",
    "ptt_df = dfs[\"ptt\"]\n",
    "mobile01_df = dfs[\"mobile01\"]\n",
    "combined_df = pd.concat([ptt_df, mobile01_df], ignore_index=True)\n",
    "\n",
    "# 載入詐騙關鍵字\n",
    "keywords_df = dfs[\"keywords\"]\n",
    "scam_keywords = keywords_df[\"詞語\"].dropna().tolist()\n",
    "\n",
    "# 結合欄位文本\n",
    "def combine_text(row):\n",
    "    content = ''\n",
    "    for col in ['發文內容', '留言內容', 'content']:\n",
    "        if col in row and pd.notna(row[col]):\n",
    "            content += str(row[col]) + ' '\n",
    "    return content.strip()\n",
    "\n",
    "combined_df['text'] = combined_df.apply(combine_text, axis=1)\n",
    "\n",
    "# 標記是否為詐騙句子\n",
    "def is_scam(text):\n",
    "    return int(any(kw in text for kw in scam_keywords))\n",
    "\n",
    "combined_df['label'] = combined_df['text'].apply(is_scam)\n",
    "\n",
    "# 篩除空文字\n",
    "combined_df = combined_df[combined_df['text'].str.strip().astype(bool)]\n",
    "\n",
    "# 中文斷詞\n",
    "def tokenize(text):\n",
    "    return \" \".join(jieba.cut(text))\n",
    "\n",
    "combined_df['text_tokenized'] = combined_df['text'].apply(tokenize)\n",
    "\n",
    "# 拆分資料\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    combined_df['text_tokenized'],\n",
    "    combined_df['label'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Tokenizer 編碼文字\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# 補齊長度\n",
    "max_len = 100\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
    "\n",
    "# 建立簡單 LSTM 模型\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=64, input_length=max_len),\n",
    "    LSTM(64, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 訓練模型（縮短為 3 epoch）\n",
    "model.fit(X_train_pad, y_train, epochs=3, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# 定義詐騙預測函數\n",
    "def predict_scam(sentence):\n",
    "    tokenized = \" \".join(jieba.cut(sentence))\n",
    "    seq = tokenizer.texts_to_sequences([tokenized])\n",
    "    padded = pad_sequences(seq, maxlen=max_len)\n",
    "    prob = model.predict(padded)[0][0]\n",
    "    return prob\n",
    "\n",
    "# 範例預測\n",
    "example_sentence = \"您中獎了！請立即回撥專線領獎\"\n",
    "example_prob = predict_scam(example_sentence)\n",
    "example_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39adbb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9997644"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定義詐騙預測函數\n",
    "def predict_scam(sentence):\n",
    "    tokenized = \" \".join(jieba.cut(sentence))\n",
    "    seq = tokenizer.texts_to_sequences([tokenized])\n",
    "    padded = pad_sequences(seq, maxlen=max_len)\n",
    "    prob = model.predict(padded)[0][0]\n",
    "    return prob\n",
    "\n",
    "# 範例預測\n",
    "example_sentence = \"\t您的帳戶異常，需立即操作ATM解除分期付款。\"\n",
    "example_prob = predict_scam(example_sentence)\n",
    "example_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e55c46cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robby1206\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m953/953\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 52ms/step - accuracy: 0.7586 - loss: 0.4747 - val_accuracy: 0.7833 - val_loss: 0.4257\n",
      "Epoch 2/3\n",
      "\u001b[1m953/953\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 52ms/step - accuracy: 0.8041 - loss: 0.4001 - val_accuracy: 0.7863 - val_loss: 0.4202\n",
      "Epoch 3/3\n",
      "\u001b[1m953/953\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 53ms/step - accuracy: 0.8174 - loss: 0.3747 - val_accuracy: 0.7882 - val_loss: 0.4246\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 344ms/step\n",
      "0.53160834\n"
     ]
    }
   ],
   "source": [
    "# ========== 1. 安裝套件（第一次用） ==========\n",
    "# pip install jieba pandas scikit-learn tensorflow openpyxl\n",
    "\n",
    "# ========== 2. 載入必要模組 ==========\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# ========== 3. 讀取資料 ==========\n",
    "ptt_df = pd.read_csv(\"DATA_DIR/ptt_語料_處理後.csv\")\n",
    "mobile_df = pd.read_csv(\"DATA_DIR/mobile01_處理後.csv\")\n",
    "keywords_df = pd.read_csv(\"DATA_DIR/500精簡詐騙字詞_UTF8.csv\")\n",
    "scam_keywords = keywords_df[\"詞語\"].dropna().tolist()\n",
    "\n",
    "df = pd.concat([ptt_df, mobile_df], ignore_index=True)\n",
    "\n",
    "# 合併內容欄\n",
    "def combine_text(row):\n",
    "    for col in ['發文內容', '留言內容', 'content']:\n",
    "        if col in row and pd.notna(row[col]):\n",
    "            return str(row[col])\n",
    "    return \"\"\n",
    "\n",
    "df[\"text\"] = df.apply(combine_text, axis=1)\n",
    "\n",
    "# 標記詐騙與否\n",
    "df[\"label\"] = df[\"text\"].apply(lambda x: int(any(k in x for k in scam_keywords)))\n",
    "df = df[df[\"text\"].str.strip().astype(bool)]\n",
    "\n",
    "# 去除關鍵字，避免資料洩漏\n",
    "def remove_keywords(text, keywords):\n",
    "    for k in keywords:\n",
    "        text = text.replace(k, \"\")\n",
    "    return text\n",
    "\n",
    "df[\"text_clean\"] = df[\"text\"].apply(lambda x: remove_keywords(x, scam_keywords))\n",
    "df[\"text_tokenized\"] = df[\"text_clean\"].apply(lambda x: \" \".join(jieba.cut(x)))\n",
    "\n",
    "# ========== 4. 建立訓練集 ==========\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text_tokenized\"], df[\"label\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# ========== 5. 向量化 ==========\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_len = 100\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
    "\n",
    "# ========== 6. 建立模型 ==========\n",
    "model = Sequential([\n",
    "    Embedding(5000, 64, input_length=max_len),\n",
    "    LSTM(64, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# ========== 7. 訓練模型 ==========\n",
    "model.fit(X_train_pad, y_train, epochs=3, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# ========== 8. 預測函式 ==========\n",
    "def predict_scam(sentence):\n",
    "    tokenized = \" \".join(jieba.cut(sentence))\n",
    "    seq = tokenizer.texts_to_sequences([tokenized])\n",
    "    padded = pad_sequences(seq, maxlen=max_len)\n",
    "    prob = model.predict(padded)[0][0]\n",
    "    return prob\n",
    "\n",
    "# 測試例句\n",
    "print(predict_scam(\"您中獎了！請立即回撥專線領獎\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9396cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "         您已中獎，請立即回撥專線領獎。」​      詐騙機率\n",
      "0   您的帳戶異常，需立即操作ATM解除分期付款。  0.935278\n",
      "1    我是檢察官，您的帳戶涉及洗錢，需配合調查。  0.972129\n",
      "2        加入我們的投資計畫，保證穩定獲利。  0.765351\n",
      "3        我們是知名企業，提供高薪兼職機會。  0.952347\n",
      "4      您的健保卡被盜用，需提供個人資料核對。  0.973686\n",
      "5     請先支付保證金，我們將立即處理您的問題。  0.855302\n",
      "6       我們公司有破解程式，能幫您追回損失。  0.956261\n",
      "7        這是限時優惠，每天僅限50個名額。  0.641756\n",
      "8   請加入我們的LINE群組，瞭解更多賺錢機會。  0.789238\n",
      "9  您好，歡迎光臨本店，請問有什麼可以為您服務的？  0.993549\n",
      "✅ 已儲存為 語句詐騙預測結果.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 載入 Excel\n",
    "df = pd.read_excel(\"語句預測.xlsx\")\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# 自動判斷句子欄位名稱\n",
    "if \"句子\" in df.columns:\n",
    "    sentence_col = \"句子\"\n",
    "else:\n",
    "    sentence_col = df.columns[0]  # 預設使用第一欄\n",
    "\n",
    "# 取得語句列表\n",
    "sentences = df[sentence_col].dropna().astype(str).tolist()\n",
    "\n",
    "# 預測函數\n",
    "def predict_scam_batch(sent_list):\n",
    "    results = []\n",
    "    for sentence in sent_list:\n",
    "        tokenized = \" \".join(jieba.cut(sentence))\n",
    "        seq = tokenizer.texts_to_sequences([tokenized])\n",
    "        padded = pad_sequences(seq, maxlen=max_len)\n",
    "        prob = model.predict(padded)[0][0]\n",
    "        results.append(prob)\n",
    "    return results\n",
    "\n",
    "# 預測\n",
    "df[\"詐騙機率\"] = predict_scam_batch(sentences)\n",
    "\n",
    "# 顯示結果\n",
    "print(df[[sentence_col, \"詐騙機率\"]].head(10))\n",
    "\n",
    "# 儲存\n",
    "df.to_excel(\"語句詐騙預測結果.xlsx\", index=False)\n",
    "print(\"✅ 已儲存為 語句詐騙預測結果.xlsx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_scraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
