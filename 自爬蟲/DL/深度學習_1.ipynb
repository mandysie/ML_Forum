{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb718bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===============================\n",
    "# # âœ… å¿…è¦å¥—ä»¶å®‰è£ (è«‹åœ¨çµ‚ç«¯æ©ŸåŸ·è¡Œ)\n",
    "# # ===============================\n",
    "# # pip install transformers datasets scikit-learn pandas torch torchvision torchaudio matplotlib seaborn openpyxl\n",
    "\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from datasets import Dataset\n",
    "# from transformers import (\n",
    "#     BertTokenizerFast,\n",
    "#     BertForSequenceClassification,\n",
    "#     TrainingArguments,\n",
    "#     Trainer,\n",
    "#     EarlyStoppingCallback,\n",
    "#     TextClassificationPipeline\n",
    "# )\n",
    "# from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "# import matplotlib.font_manager as fm # For font checking/setting\n",
    "\n",
    "# # ===============================\n",
    "# # âœ… è·¯å¾‘è¨­å®š (è«‹æ ¹æ“šä½ çš„æœ¬æ©Ÿè³‡æ–™å¤¾çµæ§‹ä¿®æ”¹)\n",
    "# # ===============================\n",
    "# DATA_DIR = \"./data/\" # å­˜æ”¾ CSV æª”æ¡ˆçš„è³‡æ–™å¤¾\n",
    "# OUTPUT_DIR = \"./output/\" # å­˜æ”¾æ¨¡å‹ã€çµæœã€åœ–ç‰‡çš„è³‡æ–™å¤¾\n",
    "\n",
    "# # ç¢ºä¿è¼¸å‡ºè³‡æ–™å¤¾å­˜åœ¨\n",
    "# os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "# os.makedirs(os.path.join(OUTPUT_DIR, \"bert_results\"), exist_ok=True)\n",
    "# os.makedirs(os.path.join(OUTPUT_DIR, \"bert_logs\"), exist_ok=True)\n",
    "# os.makedirs(os.path.join(OUTPUT_DIR, \"bert_model_chinese\"), exist_ok=True)\n",
    "\n",
    "# # CSV æª”æ¡ˆè·¯å¾‘\n",
    "# FILE_DF1 = os.path.join(DATA_DIR, 'finfo_posts_ç”¢éšª_å£½éšª_æŠ•è³‡å‹.csv')\n",
    "# FILE_DF2 = os.path.join(DATA_DIR, 'mobile01_è™•ç†å¾Œ.csv')\n",
    "# FILE_DF3 = os.path.join(DATA_DIR, 'ptt_èªæ–™_è™•ç†å¾Œ.csv')\n",
    "\n",
    "# # è¼¸å‡ºæª”æ¡ˆè·¯å¾‘\n",
    "# MODEL_OUTPUT_DIR = os.path.join(OUTPUT_DIR, \"bert_results\")\n",
    "# LOGGING_DIR = os.path.join(OUTPUT_DIR, \"bert_logs\")\n",
    "# SAVED_MODEL_DIR = os.path.join(OUTPUT_DIR, \"bert_model_chinese\")\n",
    "# PREDICTION_CSV_PATH = os.path.join(OUTPUT_DIR, \"prediction_result.csv\")\n",
    "# PLOT_PATH = os.path.join(OUTPUT_DIR, \"prediction_probability_distribution.png\")\n",
    "\n",
    "\n",
    "# # ===============================\n",
    "# # âœ… è¨­å®šï¼šå¿«é€Ÿè¨“ç·´æ¨¡å¼é–‹é—œ\n",
    "# # ===============================\n",
    "# QUICK_MODE = True  # True âœ æŠ½æ¨£å¿«é€Ÿè¨“ç·´ï¼ŒFalse âœ å…¨è³‡æ–™\n",
    "\n",
    "# # ===============================\n",
    "# # âœ… è¼‰å…¥è³‡æ–™ï¼ˆå«æ¬„ä½æ¸…ç†ï¼‰\n",
    "# # ===============================\n",
    "# print(\"ğŸ”„ é–‹å§‹è¼‰å…¥èˆ‡è™•ç†è³‡æ–™...\")\n",
    "# try:\n",
    "#     df1 = pd.read_csv(\"DATA_DIR/finfo_posts_ç”¢éšª_å£½éšª_æŠ•è³‡å‹.csv\")\n",
    "#     df2 = pd.read_csv(\"DATA_DIR/mobile01_è™•ç†å¾Œ.csv\")\n",
    "#     df3 = pd.read_csv(\"DATA_DIR/ptt_èªæ–™_è™•ç†å¾Œ.csv\")\n",
    "# except FileNotFoundError as e:\n",
    "#     print(f\"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°è³‡æ–™æª”æ¡ˆï¼è«‹ç¢ºèªè·¯å¾‘ '{DATA_DIR}' ä¸‹æœ‰æŒ‡å®šçš„ CSV æª”æ¡ˆã€‚\")\n",
    "#     print(f\"è©³ç´°éŒ¯èª¤ï¼š{e}\")\n",
    "#     exit() # æ‰¾ä¸åˆ°æª”æ¡ˆç›´æ¥çµæŸç¨‹å¼\n",
    "\n",
    "# for df in [df1, df2, df3]:\n",
    "#     df.columns = df.columns.str.strip()\n",
    "\n",
    "# def extract_text(row):\n",
    "#     text = ''\n",
    "#     # å„ªå…ˆä½¿ç”¨ 'ç™¼æ–‡å…§å®¹'/'ç•™è¨€å…§å®¹' æˆ– 'content'\n",
    "#     if 'content' in row and pd.notna(row['content']):\n",
    "#          text += str(row['content']) + ' '\n",
    "#     elif 'ç™¼æ–‡å…§å®¹' in row and pd.notna(row['ç™¼æ–‡å…§å®¹']):\n",
    "#         text += str(row['ç™¼æ–‡å…§å®¹']) + ' '\n",
    "#         # Finfos å¯èƒ½åŒæ™‚æœ‰ç™¼æ–‡å’Œç•™è¨€ï¼Œéƒ½åŠ é€²å»\n",
    "#         if 'ç•™è¨€å…§å®¹' in row and pd.notna(row['ç•™è¨€å…§å®¹']):\n",
    "#            text += str(row['ç•™è¨€å…§å®¹'])\n",
    "\n",
    "#     # å¦‚æœä¸Šé¢éƒ½æ²’æœ‰ï¼Œå˜—è©¦å…¶ä»–å¯èƒ½çš„æ¬„ä½ï¼ˆæ ¹æ“šå¯¦éš›è³‡æ–™èª¿æ•´ï¼‰\n",
    "#     # elif 'å…¶ä»–æ¬„ä½' in row and pd.notna(row['å…¶ä»–æ¬„ä½']):\n",
    "#     #     text += str(row['å…¶ä»–æ¬„ä½'])\n",
    "\n",
    "#     return text.strip()\n",
    "\n",
    "# def prepare_labeled_df(df):\n",
    "#     df['text'] = df.apply(extract_text, axis=1)\n",
    "#     # ç¢ºä¿ 'è©é¨™é—œéµè©æ¬¡æ•¸' æ¬„ä½å­˜åœ¨ä¸”ç‚ºæ•¸å€¼ï¼Œè‹¥ä¸å­˜åœ¨æˆ–éæ•¸å€¼å‰‡é è¨­ç‚º0\n",
    "#     if 'è©é¨™é—œéµè©æ¬¡æ•¸' not in df.columns:\n",
    "#         print(f\"âš ï¸ è­¦å‘Šï¼šè³‡æ–™é›†ä¸­ç¼ºå°‘ 'è©é¨™é—œéµè©æ¬¡æ•¸' æ¬„ä½ï¼Œå°‡å‡è¨­æ‰€æœ‰ label ç‚º 0ã€‚\")\n",
    "#         df['label'] = 0\n",
    "#     else:\n",
    "#         # å˜—è©¦è½‰æ›ç‚ºæ•¸å€¼ï¼Œç„¡æ³•è½‰æ›çš„è¨­ç‚º 0\n",
    "#         df['è©é¨™é—œéµè©æ¬¡æ•¸'] = pd.to_numeric(df['è©é¨™é—œéµè©æ¬¡æ•¸'], errors='coerce').fillna(0)\n",
    "#         df['label'] = df['è©é¨™é—œéµè©æ¬¡æ•¸'].apply(lambda x: 1 if x > 0 else 0)\n",
    "#     return df[['text', 'label']]\n",
    "\n",
    "# # è™•ç† df2 å’Œ df3 ä½œç‚ºè¨“ç·´è³‡æ–™\n",
    "# df_train_list = []\n",
    "# print(\"è™•ç† df2 (Mobile01)...\")\n",
    "# df_train_list.append(prepare_labeled_df(df2))\n",
    "# print(\"è™•ç† df3 (PTT)...\")\n",
    "# df_train_list.append(prepare_labeled_df(df3))\n",
    "# df_train = pd.concat(df_train_list, ignore_index=True)\n",
    "\n",
    "# # è™•ç† df1 ä½œç‚ºé æ¸¬è³‡æ–™\n",
    "# print(\"è™•ç† df1 (Finfo)...\")\n",
    "# df_unlabeled = df1.copy()\n",
    "# df_unlabeled['text'] = df_unlabeled.apply(extract_text, axis=1)\n",
    "\n",
    "# # ç§»é™¤ text ç‚ºç©ºå­—ä¸²çš„è³‡æ–™ (é¿å… Tokenizer å‡ºéŒ¯)\n",
    "# df_train = df_train[df_train['text'].str.strip() != '']\n",
    "# df_unlabeled = df_unlabeled[df_unlabeled['text'].str.strip() != '']\n",
    "\n",
    "# # ğŸ”¹ å¿«é€Ÿæ¨¡å¼ï¼šæŠ½æ¨£è¨“ç·´è³‡æ–™\n",
    "# if QUICK_MODE:\n",
    "#     n_samples = min(10000, len(df_train)) # é˜²æ­¢ç¸½è³‡æ–™å°‘æ–¼ä¸€è¬ç­†è€Œå‡ºéŒ¯\n",
    "#     if len(df_train) > n_samples:\n",
    "#         df_train = df_train.sample(n=n_samples, random_state=42)\n",
    "#         print(f\"âš¡ å¿«é€Ÿæ¨¡å¼å•Ÿç”¨ï¼šæŠ½æ¨£ {n_samples} ç­†è³‡æ–™è¨“ç·´\")\n",
    "#     else:\n",
    "#         print(f\"âš¡ å¿«é€Ÿæ¨¡å¼å•Ÿç”¨ï¼šä½¿ç”¨å…¨éƒ¨ {len(df_train)} ç­†è³‡æ–™è¨“ç·´ (ç¸½æ•¸å°‘æ–¼æˆ–ç­‰æ–¼æŠ½æ¨£æ•¸)\")\n",
    "\n",
    "\n",
    "# print(f\"âœ… æ¸…ç†èˆ‡æº–å‚™å®Œæˆï¼š\")\n",
    "# print(f\"   è¨“ç·´æ¨£æœ¬æ•¸ï¼š{len(df_train)}\")\n",
    "# print(f\"   é æ¸¬æ¨£æœ¬æ•¸ï¼š{len(df_unlabeled)}\")\n",
    "# print(f\"   è¨“ç·´è³‡æ–™æ¨™ç±¤åˆ†ä½ˆ:\\n{df_train['label'].value_counts(normalize=True)}\")\n",
    "\n",
    "\n",
    "# # ===============================\n",
    "# # âœ… tokenizer + åˆ‡åˆ† Dataset\n",
    "# # ===============================\n",
    "# print(\"\\nğŸ”„ é–‹å§‹ Tokenization èˆ‡è³‡æ–™é›†æº–å‚™...\")\n",
    "# # ... (tokenizer è¼‰å…¥, tokenize_function å®šç¾©, Dataset.from_pandas, map ç­‰ä¸è®Š) ...\n",
    "\n",
    "# try:\n",
    "#     raw_dataset = Dataset.from_pandas(df_train)\n",
    "#     # ç§»é™¤åŸå§‹ text ç¯€çœè¨˜æ†¶é«”, ç¢ºä¿ 'label' æ¬„ä½å­˜åœ¨ä»¥ä¾¿å¾ŒçºŒè½‰æ›\n",
    "#     tokenized_dataset = raw_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
    "#     tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\") # å…ˆæ”¹å\n",
    "#     tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "#     # --- ğŸ‘‡ğŸ‘‡ğŸ‘‡ æ–°å¢ï¼šè½‰æ› labels æ¬„ä½ç‚º ClassLabel ğŸ‘‡ğŸ‘‡ğŸ‘‡ ---\n",
    "#     from datasets import ClassLabel, Features # ç¢ºä¿å°å…¥\n",
    "#     print(\"ğŸ”„ Casting 'labels' column to ClassLabel for stratification...\")\n",
    "#     # å®šç¾©æ–°çš„ Featuresï¼Œæ˜ç¢ºæŒ‡å®š 'labels' çš„é¡å‹\n",
    "#     new_features = tokenized_dataset.features.copy()\n",
    "#     new_features['labels'] = ClassLabel(num_classes=2, names=['not_scam', 'scam']) # num_classes=2 å› ç‚ºæ˜¯äºŒå…ƒåˆ†é¡\n",
    "#     tokenized_dataset = tokenized_dataset.cast(new_features)\n",
    "#     print(f\"âœ… 'labels' column type after casting: {tokenized_dataset.features['labels']}\")\n",
    "#     # --- ğŸ‘†ğŸ‘†ğŸ‘† æ–°å¢çµæŸ ğŸ‘†ğŸ‘†ğŸ‘† ---\n",
    "\n",
    "\n",
    "#     # ç¾åœ¨å¯ä»¥é€²è¡Œåˆ†å±¤æŠ½æ¨£äº†\n",
    "#     split_dataset = tokenized_dataset.train_test_split(\n",
    "#         test_size=0.2,\n",
    "#         seed=42,\n",
    "#         stratify_by_column=\"labels\" # ç¾åœ¨ labels æ˜¯ ClassLabelï¼Œå¯ä»¥åˆ†å±¤äº†\n",
    "#     )\n",
    "#     train_dataset = split_dataset[\"train\"]\n",
    "#     eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "#     print(f\"âœ… Tokenization èˆ‡è³‡æ–™é›†åˆ‡åˆ†å®Œæˆï¼š\")\n",
    "#     print(f\"   Train size: {len(train_dataset)}\")\n",
    "#     print(f\"   Eval size: {len(eval_dataset)}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"âŒ Tokenization æˆ–è³‡æ–™é›†è½‰æ›/åˆ‡åˆ†å¤±æ•—ï¼š{e}\")\n",
    "#     # æ‰“å°æ›´è©³ç´°çš„è¿½è¹¤ä¿¡æ¯ï¼Œæœ‰åŠ©æ–¼é™¤éŒ¯\n",
    "#     import traceback\n",
    "#     traceback.print_exc()\n",
    "#     exit()\n",
    "# # ===============================\n",
    "# # âœ… æ¨¡å‹ + è¨“ç·´è¨­å®š + EarlyStopping\n",
    "# # ===============================\n",
    "# print(\"\\nğŸ”„ é–‹å§‹è¨­å®šæ¨¡å‹èˆ‡è¨“ç·´åƒæ•¸...\")\n",
    "# model = BertForSequenceClassification.from_pretrained(\"bert-base-chinese\", num_labels=2)\n",
    "\n",
    "# # --- è¨“ç·´åƒæ•¸ ---\n",
    "# # !! æ³¨æ„ !! å¦‚æœé‡åˆ° CUDA Out of Memory (OOM) éŒ¯èª¤ï¼Œè«‹å„ªå…ˆé™ä½ per_device_train_batch_size\n",
    "# # GTX 1060 6GB åœ¨ max_length=256 æ™‚ï¼Œbatch_size å¯èƒ½éœ€è¦è¨­ç‚º 4 æˆ– 2\n",
    "# # å¦‚æœ max_length=128ï¼Œbatch_size=8 æˆ–è¨±å¯ä»¥ï¼Œä½†ä»éœ€å˜—è©¦\n",
    "# TRAIN_BATCH_SIZE = 4 # é™ä½ batch size ä»¥é©æ‡‰ GTX 1060 (å¯å˜—è©¦ 8, 4, 2)\n",
    "# EVAL_BATCH_SIZE = 8 # è©•ä¼°æ™‚ batch size é€šå¸¸å¯ä»¥ç¨å¤§ (å¯å˜—è©¦ 16, 8)\n",
    "# NUM_EPOCHS = 3 if QUICK_MODE else 10\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=MODEL_OUTPUT_DIR,\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "#     per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "#     num_train_epochs=NUM_EPOCHS,\n",
    "#     weight_decay=0.01,\n",
    "#     load_best_model_at_end=True,\n",
    "#     logging_dir=LOGGING_DIR,\n",
    "#     logging_strategy=\"epoch\",\n",
    "#     metric_for_best_model=\"eval_f1\", # æ”¹ç”¨ F1 ä½œç‚ºä¸»è¦è©•ä¼°æŒ‡æ¨™å¯èƒ½æ›´é©åˆä¸å¹³è¡¡è³‡æ–™\n",
    "#     save_total_limit=1,          # åªä¿ç•™æœ€å¥½çš„æ¨¡å‹\n",
    "#     report_to=\"none\",            # é—œæ‰ wandb ç­‰å¤–éƒ¨å ±å‘Š\n",
    "#     fp16=torch.cuda.is_available(), # âœ… è‡ªå‹•å•Ÿç”¨æ··åˆç²¾åº¦ (å¦‚æœ CUDA å¯ç”¨)\n",
    "#     gradient_accumulation_steps=2, # æ¢¯åº¦ç´¯ç©ï¼Œç­‰æ•ˆ batch size = TRAIN_BATCH_SIZE * 2ï¼Œå¯ç¨å¾®å½Œè£œå° batch size\n",
    "#     learning_rate=3e-5,          # å¸¸ç”¨çš„å­¸ç¿’ç‡\n",
    "# )\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     preds = np.argmax(logits, axis=-1)\n",
    "#     precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "#     acc = accuracy_score(labels, preds)\n",
    "#     return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=eval_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     callbacks=[EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.001)] # åŠ å…¥ threshold é¿å…å¾®å°æ”¹å–„ä¹Ÿç¹¼çºŒç­‰\n",
    "# )\n",
    "\n",
    "# print(f\"âœ… è¨­å®šå®Œæˆï¼Œæº–å‚™é–‹å§‹è¨“ç·´...\")\n",
    "# print(f\"   ä½¿ç”¨ GPU: {torch.cuda.is_available()}\")\n",
    "# if torch.cuda.is_available():\n",
    "#     print(f\"   CUDA è£ç½®: {torch.cuda.get_device_name(0)}\")\n",
    "# print(f\"   è¨“ç·´ Epochs: {NUM_EPOCHS}\")\n",
    "# print(f\"   è¨“ç·´ Batch Size (Per Device): {TRAIN_BATCH_SIZE}\")\n",
    "# print(f\"   æ¢¯åº¦ç´¯ç©æ­¥æ•¸: {training_args.gradient_accumulation_steps}\")\n",
    "# print(f\"   ç­‰æ•ˆ Batch Size: {TRAIN_BATCH_SIZE * training_args.gradient_accumulation_steps}\")\n",
    "# print(f\"   è©•ä¼° Batch Size (Per Device): {EVAL_BATCH_SIZE}\")\n",
    "# print(f\"   æ··åˆç²¾åº¦ (FP16): {training_args.fp16}\")\n",
    "\n",
    "# # ===============================\n",
    "# # âœ… æ¨¡å‹è¨“ç·´\n",
    "# # ===============================\n",
    "# print(\"\\nğŸ‹ï¸â€â™‚ï¸ é–‹å§‹è¨“ç·´æ¨¡å‹...\")\n",
    "# try:\n",
    "#     trainer.train()\n",
    "#     print(\"âœ… è¨“ç·´å®Œæˆï¼\")\n",
    "# except RuntimeError as e:\n",
    "#     if \"CUDA out of memory\" in str(e):\n",
    "#         print(\"\\nâŒ éŒ¯èª¤ï¼šCUDA Out of Memoryï¼\")\n",
    "#         print(\"ğŸ§  å»ºè­°ï¼šè«‹å˜—è©¦é™ä½ TrainingArguments ä¸­çš„ 'per_device_train_batch_size'ã€‚\")\n",
    "#         print(f\"   ç›®å‰çš„è¨­å®šæ˜¯ {TRAIN_BATCH_SIZE}ï¼Œå¯ä»¥è©¦è©¦ 4 æˆ– 2ã€‚\")\n",
    "#         print(f\"   æˆ–è€…æ¸›å°‘ tokenize_function ä¸­çš„ 'max_length' (ç›®å‰æ˜¯ {tokenizer.model_max_length})ã€‚\")\n",
    "#     else:\n",
    "#         print(f\"\\nâŒ è¨“ç·´éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š{e}\")\n",
    "#     exit()\n",
    "# except Exception as e:\n",
    "#     print(f\"\\nâŒ è¨“ç·´éç¨‹ä¸­ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤ï¼š{e}\")\n",
    "#     exit()\n",
    "\n",
    "\n",
    "# # ===============================\n",
    "# # âœ… å„²å­˜æ¨¡å‹\n",
    "# # ===============================\n",
    "# print(f\"\\nğŸ’¾ æ­£åœ¨å„²å­˜æœ€ä½³æ¨¡å‹è‡³ï¼š{SAVED_MODEL_DIR}\")\n",
    "# trainer.save_model(SAVED_MODEL_DIR)\n",
    "# tokenizer.save_pretrained(SAVED_MODEL_DIR)\n",
    "# print(\"âœ… æ¨¡å‹èˆ‡ Tokenizer å„²å­˜å®Œæˆã€‚\")\n",
    "\n",
    "# # ===============================\n",
    "# # âœ… é æ¸¬æµç¨‹ + è¼¸å‡º CSV\n",
    "# # ===============================\n",
    "# print(\"\\nğŸ“Š é–‹å§‹ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹é€²è¡Œé æ¸¬...\")\n",
    "\n",
    "# # é‡æ–°è¼‰å…¥æœ€ä½³æ¨¡å‹ä»¥ç¢ºä¿ä½¿ç”¨çš„æ˜¯å„²å­˜çš„ç‰ˆæœ¬\n",
    "# model = BertForSequenceClassification.from_pretrained(SAVED_MODEL_DIR)\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(SAVED_MODEL_DIR)\n",
    "\n",
    "# # è¨­å®š pipeline device\n",
    "# device_num = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# pipe = TextClassificationPipeline(\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     return_all_scores=True, # ç²å–æ‰€æœ‰é¡åˆ¥åˆ†æ•¸\n",
    "#     device=device_num       # ä½¿ç”¨ GPU (å¦‚æœå¯ç”¨)\n",
    "# )\n",
    "\n",
    "# # åŸ·è¡Œé æ¸¬ (å¦‚æœè³‡æ–™é‡å¤§ï¼Œé€™ä¸€æ­¥å¯èƒ½ä¹Ÿéœ€è¦ä¸€äº›æ™‚é–“)\n",
    "# # ä½¿ç”¨ tolist() é¿å…å‚³é Series ç‰©ä»¶å¯èƒ½é€ æˆçš„å•é¡Œ\n",
    "# texts_to_predict = df_unlabeled[\"text\"].tolist()\n",
    "\n",
    "# # åˆ†æ‰¹é æ¸¬ä»¥æ§åˆ¶è¨˜æ†¶é«”ä½¿ç”¨\n",
    "# batch_size = 32 # å¯ä»¥æ ¹æ“šä½ çš„ RAM å’Œ VRAM èª¿æ•´\n",
    "# results = []\n",
    "# print(f\"   é æ¸¬ç¸½ç­†æ•¸: {len(texts_to_predict)}ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}\")\n",
    "# for i in range(0, len(texts_to_predict), batch_size):\n",
    "#     batch_texts = texts_to_predict[i:i + batch_size]\n",
    "#     batch_results = pipe(batch_texts)\n",
    "#     results.extend(batch_results)\n",
    "#     print(f\"   å·²é æ¸¬ {min(i + batch_size, len(texts_to_predict))}/{len(texts_to_predict)} ç­†\")\n",
    "\n",
    "\n",
    "# # è™•ç†é æ¸¬çµæœ\n",
    "# # å‡è¨­ Label 'LABEL_1' ä»£è¡¨è©é¨™ (éœ€æ ¹æ“š pipeline å¯¦éš›è¼¸å‡ºç¢ºèª)\n",
    "# pred_labels = []\n",
    "# pred_probs = []\n",
    "# for res in results:\n",
    "#     score_label_1 = 0.0\n",
    "#     highest_score_label = 0\n",
    "#     highest_score = 0.0\n",
    "#     for item in res:\n",
    "#         if item['label'] == 'LABEL_1':\n",
    "#             score_label_1 = item['score']\n",
    "#         if item['score'] > highest_score:\n",
    "#             highest_score = item['score']\n",
    "#             # å¾ 'LABEL_X' ä¸­å–å‡ºæ•¸å­— X\n",
    "#             highest_score_label = int(item['label'].split('_')[-1])\n",
    "\n",
    "#     pred_labels.append(highest_score_label)\n",
    "#     pred_probs.append(round(score_label_1, 4)) # å„²å­˜ LABEL_1 çš„æ©Ÿç‡\n",
    "\n",
    "# df_unlabeled[\"predicted_label\"] = pred_labels\n",
    "# df_unlabeled[\"predicted_prob_scam\"] = pred_probs # æ¬„ä½åæ”¹ç‚ºæ›´æ¸…æ™°\n",
    "\n",
    "# # é¸æ“‡è¦è¼¸å‡ºçš„æ¬„ä½ (åªä¿ç•™å¿…è¦æ¬„ä½ï¼Œé¿å…æª”æ¡ˆéå¤§)\n",
    "# output_columns = ['text', 'predicted_label', 'predicted_prob_scam']\n",
    "# # å¦‚æœåŸå§‹ df1 æœ‰ ID æˆ–å…¶ä»–é‡è¦æ¬„ä½ï¼Œä¹Ÿå¯ä»¥åŠ å…¥\n",
    "# # if 'id_column' in df_unlabeled.columns:\n",
    "# #     output_columns.insert(0, 'id_column')\n",
    "\n",
    "# df_unlabeled[output_columns].to_csv(PREDICTION_CSV_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# print(f\"âœ… å®Œæˆé æ¸¬ï¼çµæœå„²å­˜æ–¼ï¼š{PREDICTION_CSV_PATH}\")\n",
    "\n",
    "\n",
    "# # ===============================\n",
    "# # âœ… (æ–°å¢) ç¹ªè£½é æ¸¬æ©Ÿç‡åˆ†ä½ˆåœ–\n",
    "# # ===============================\n",
    "# print(\"\\nğŸ¨ æ­£åœ¨ç¹ªè£½é æ¸¬æ©Ÿç‡åˆ†ä½ˆåœ–...\")\n",
    "\n",
    "# # --- å­—é«”è¨­å®š (é‡å°æœ¬æ©Ÿ Matplotlib) ---\n",
    "# # Windows: C:/Windows/Fonts/msjh.ttc (å¾®è»Ÿæ­£é»‘é«”)\n",
    "# # macOS: /System/Library/Fonts/STHeiti Medium.ttc (é»‘é«”-ç¹ ä¸­ç­‰)\n",
    "# # Linux: å¯èƒ½éœ€è¦å…ˆå®‰è£å­—å‹ (å¦‚ Noto Sans CJK TC) ä¸¦æ‰¾åˆ° .ttf æˆ– .otf æª”\n",
    "# # å¦‚æœæ‰¾ä¸åˆ°å­—é«”æˆ–ä¸ç¢ºå®šï¼Œå¯ä»¥è¨»è§£æ‰ä¸‹é¢å¹¾è¡Œï¼Œä½†ä¸­æ–‡å¯èƒ½é¡¯ç¤ºç‚ºæ–¹å¡Š\n",
    "# try:\n",
    "#     # å˜—è©¦è¨­å®šä¸­æ–‡å­—é«” (è«‹æ ¹æ“šä½ çš„ä½œæ¥­ç³»çµ±ä¿®æ”¹è·¯å¾‘æˆ–å­—é«”åç¨±)\n",
    "#     font_path = None\n",
    "#     if os.path.exists(\"C:/Windows/Fonts/msjh.ttc\"):\n",
    "#         font_path = \"C:/Windows/Fonts/msjh.ttc\"\n",
    "#         font_prop = fm.FontProperties(fname=font_path)\n",
    "#         plt.rcParams['font.family'] = font_prop.get_name()\n",
    "#     elif os.path.exists(\"/System/Library/Fonts/STHeiti Medium.ttc\"):\n",
    "#          # macOS å¯èƒ½å¯ä»¥ç›´æ¥è¨­å®šå­—é«”åç¨±\n",
    "#          plt.rcParams['font.sans-serif'] = ['Heiti TC'] # æˆ– 'PingFang TC'\n",
    "#     elif os.path.exists(\"/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc\"): # ç¯„ä¾‹ Linux è·¯å¾‘\n",
    "#         font_path = \"/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc\"\n",
    "#         font_prop = fm.FontProperties(fname=font_path)\n",
    "#         plt.rcParams['font.family'] = font_prop.get_name()\n",
    "#     else:\n",
    "#         print(\"âš ï¸ æœªæ‰¾åˆ°æŒ‡å®šçš„ä¸­æ–‡å­—é«”ï¼Œåœ–è¡¨ä¸­çš„ä¸­æ–‡å¯èƒ½ç„¡æ³•æ­£å¸¸é¡¯ç¤ºã€‚\")\n",
    "\n",
    "#     plt.rcParams['axes.unicode_minus'] = False # è§£æ±ºè² è™Ÿé¡¯ç¤ºå•é¡Œ\n",
    "# except Exception as font_e:\n",
    "#     print(f\"âš ï¸ è¨­å®šä¸­æ–‡å­—é«”æ™‚ç™¼ç”ŸéŒ¯èª¤: {font_e}. ä¸­æ–‡å¯èƒ½ç„¡æ³•æ­£å¸¸é¡¯ç¤ºã€‚\")\n",
    "# # --- å­—é«”è¨­å®šçµæŸ ---\n",
    "\n",
    "# plt.figure(figsize=(12, 7))\n",
    "# sns.histplot(data=df_unlabeled, x=\"predicted_prob_scam\", kde=True, bins=50, color=\"skyblue\") # ä½¿ç”¨ 50 å€‹å€é–“\n",
    "# plt.title(\"é æ¸¬ç‚ºè©é¨™ (Label=1) çš„æ©Ÿç‡åˆ†ä½ˆ\", fontsize=16)\n",
    "# plt.xlabel(\"æ¨¡å‹é æ¸¬ç‚ºè©é¨™çš„æ©Ÿç‡å€¼\", fontsize=12)\n",
    "# plt.ylabel(\"è³‡æ–™ç­†æ•¸\", fontsize=12)\n",
    "# plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# # åŠ ä¸Šé–¾å€¼ç·š\n",
    "# threshold = 0.5\n",
    "# plt.axvline(x=threshold, color='r', linestyle='--', label=f'åˆ†é¡é–¾å€¼ = {threshold}')\n",
    "\n",
    "# # åœ¨åœ–ä¸Šé¡¯ç¤ºä¸€äº›çµ±è¨ˆæ•¸æ“š\n",
    "# mean_prob = df_unlabeled[\"predicted_prob_scam\"].mean()\n",
    "# median_prob = df_unlabeled[\"predicted_prob_scam\"].median()\n",
    "# plt.axvline(x=mean_prob, color='g', linestyle=':', label=f'å¹³å‡æ©Ÿç‡ = {mean_prob:.3f}')\n",
    "# plt.legend(fontsize=10)\n",
    "\n",
    "# plt.tight_layout() # è‡ªå‹•èª¿æ•´é‚Šè·\n",
    "\n",
    "# # å„²å­˜åœ–ç‰‡\n",
    "# plt.savefig(PLOT_PATH, dpi=300) # æé«˜è§£æåº¦\n",
    "# # plt.show() # å¦‚æœå¸Œæœ›åœ¨ VS Code ä¸­å½ˆå‡ºè¦–çª—é¡¯ç¤ºåœ–è¡¨ï¼Œå–æ¶ˆæ­¤è¡Œè¨»è§£\n",
    "\n",
    "# print(f\"âœ… æ©Ÿç‡åˆ†ä½ˆåœ–å„²å­˜æ–¼ï¼š{PLOT_PATH}\")\n",
    "\n",
    "\n",
    "# # ===============================\n",
    "# # âœ… (æ–°å¢) ç°¡å–®è§£é‡‹\n",
    "# # ===============================\n",
    "# print(\"\\n===== ğŸ“Š åœ–è¡¨èˆ‡çµæœè§£é‡‹ =====\")\n",
    "# print(f\"æ¨¡å‹å·²å° {len(df_unlabeled)} ç­†ä¾†è‡ª '{os.path.basename(FILE_DF1)}' çš„è³‡æ–™é€²è¡Œäº†é æ¸¬ã€‚\")\n",
    "# print(f\"è©³ç´°é æ¸¬çµæœï¼ˆå«åŸå§‹æ–‡æœ¬ã€é æ¸¬æ¨™ç±¤ã€è©é¨™æ©Ÿç‡ï¼‰å„²å­˜åœ¨ï¼š\\n{PREDICTION_CSV_PATH}\")\n",
    "# print(f\"\\né æ¸¬æ©Ÿç‡åˆ†ä½ˆåœ–å„²å­˜åœ¨ï¼š\\n{PLOT_PATH}\")\n",
    "# print(\"é€™å¼µåœ–é¡¯ç¤ºäº†æ¨¡å‹å°‡æ¯ç­†è³‡æ–™åˆ¤æ–·ç‚ºã€Œè©é¨™ã€(Label=1) çš„ä¿¡å¿ƒç¨‹åº¦ï¼ˆæ©Ÿç‡å€¼ï¼‰çš„åˆ†ä½ˆæƒ…æ³ã€‚\")\n",
    "# print(\"\\nåœ–è¡¨è§€å¯Ÿé‡é»ï¼š\")\n",
    "# print(\"1. ã€å½¢ç‹€ã€‘æ©Ÿç‡åˆ†ä½ˆæ˜¯å¦å‘ˆç¾ã€é›™å³°ã€ï¼Ÿ\")\n",
    "# print(\"   - è‹¥å¤§é‡è³‡æ–™çš„æ©Ÿç‡é›†ä¸­åœ¨æ¥è¿‘ 0 å’Œæ¥è¿‘ 1 çš„å…©ç«¯ï¼Œè¡¨ç¤ºæ¨¡å‹å°å¤§éƒ¨åˆ†æ¨£æœ¬çš„åˆ¤æ–·æ¯”è¼ƒã€æœ‰ä¿¡å¿ƒã€ï¼ˆæ˜ç¢ºåˆ¤æ–·ç‚ºéè©é¨™æˆ–è©é¨™ï¼‰ã€‚\")\n",
    "# print(\"   - è‹¥æ©Ÿç‡é›†ä¸­åœ¨ä¸­é–“ï¼ˆå¦‚ 0.5 é™„è¿‘ï¼‰ï¼Œè¡¨ç¤ºæ¨¡å‹å°å¾ˆå¤šæ¨£æœ¬çš„åˆ¤æ–·æ¯”è¼ƒã€æ¨¡ç³Šã€æˆ–ã€ä¸ç¢ºå®šã€ã€‚\")\n",
    "# print(\"2. ã€é‡å¿ƒã€‘æ©Ÿç‡ä¸»è¦é›†ä¸­åœ¨å“ªä¸€é‚Šï¼Ÿ\")\n",
    "# print(f\"   - å¹³å‡æ©Ÿç‡ ({mean_prob:.3f}) å’Œä¸­ä½æ•¸æ©Ÿç‡ ({median_prob:.3f}) å¯ä»¥æä¾›åƒè€ƒã€‚\")\n",
    "# print(\"   - å¦‚æœå¤§éƒ¨åˆ†æ©Ÿç‡å€¼éƒ½åä½ï¼ˆæ¥è¿‘ 0ï¼‰ï¼Œè¡¨ç¤ºæ¨¡å‹å‚¾å‘æ–¼èªç‚ºå¤§éƒ¨åˆ†æ–‡æœ¬ã€ä¸æ˜¯ã€è©é¨™ã€‚\")\n",
    "# print(\"   - å¦‚æœå¤§éƒ¨åˆ†æ©Ÿç‡å€¼éƒ½åé«˜ï¼ˆæ¥è¿‘ 1ï¼‰ï¼Œè¡¨ç¤ºæ¨¡å‹å‚¾å‘æ–¼èªç‚ºå¤§éƒ¨åˆ†æ–‡æœ¬ã€æ˜¯ã€è©é¨™ã€‚\")\n",
    "# print(\"3. ã€é–¾å€¼é™„è¿‘ã€‘ç´…è‰²è™›ç·šï¼ˆé è¨­ 0.5ï¼‰æ˜¯å¸¸ç”¨çš„åˆ†é¡ç•Œç·šã€‚\")\n",
    "# print(\"   - åœ¨é€™æ¢ç·šé™„è¿‘çš„æ¨£æœ¬ï¼Œæ˜¯æ¨¡å‹åˆ¤æ–·çµæœã€æ¨¡æ£±å…©å¯ã€çš„å€åŸŸã€‚æ”¹è®Šé–¾å€¼æœƒå½±éŸ¿æœ€çµ‚å“ªäº›æ¨£æœ¬è¢«æ­¸é¡ç‚ºè©é¨™ã€‚\")\n",
    "# print(\"   - é€™äº›é–¾å€¼é™„è¿‘çš„æ¨£æœ¬å¯èƒ½æœ€éœ€è¦äººå·¥è¤‡æŸ¥ã€‚\")\n",
    "# print(\"\\nğŸ‘‰ å»ºè­°ï¼šæ‰“é–‹ CSV æª”æ¡ˆï¼Œå°ç…§é«˜æ©Ÿç‡ã€ä½æ©Ÿç‡ä»¥åŠæ©Ÿç‡åœ¨é–¾å€¼é™„è¿‘çš„æ–‡æœ¬ï¼Œæª¢æŸ¥æ¨¡å‹çš„åˆ¤æ–·æ˜¯å¦ç¬¦åˆé æœŸã€‚\")\n",
    "# print(\"==============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23b2101d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ROBBY1~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.797 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robby1206\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m953/953\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 52ms/step - accuracy: 0.8636 - loss: 0.3040 - val_accuracy: 0.9562 - val_loss: 0.1241\n",
      "Epoch 2/3\n",
      "\u001b[1m953/953\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 53ms/step - accuracy: 0.9636 - loss: 0.1051 - val_accuracy: 0.9585 - val_loss: 0.1232\n",
      "Epoch 3/3\n",
      "\u001b[1m953/953\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 54ms/step - accuracy: 0.9685 - loss: 0.0939 - val_accuracy: 0.9567 - val_loss: 0.1257\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 610ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9780651"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# é‡æ–°è¼‰å…¥æ‰€æœ‰å¿…è¦çš„æª”æ¡ˆèˆ‡å¥—ä»¶ï¼ˆåŒ…æ‹¬ jiebaï¼‰\n",
    "!pip install jieba --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# æª”æ¡ˆè·¯å¾‘\n",
    "file_paths = {\n",
    "    \"mobile01\": \"DATA_DIR/mobile01_è™•ç†å¾Œ.csv\",\n",
    "    \"ptt\": \"DATA_DIR/ptt_èªæ–™_è™•ç†å¾Œ.csv\",\n",
    "    \"finfo\": \"DATA_DIR/finfo_posts_ç”¢éšª_å£½éšª_æŠ•è³‡å‹.csv\",\n",
    "    \"keywords\": \"DATA_DIR/500ç²¾ç°¡è©é¨™å­—è©_UTF8.csv\"\n",
    "}\n",
    "\n",
    "# è¼‰å…¥è³‡æ–™\n",
    "dfs = {name: pd.read_csv(path) for name, path in file_paths.items()}\n",
    "\n",
    "# åˆä½µ PTT èˆ‡ Mobile01 è³‡æ–™\n",
    "ptt_df = dfs[\"ptt\"]\n",
    "mobile01_df = dfs[\"mobile01\"]\n",
    "combined_df = pd.concat([ptt_df, mobile01_df], ignore_index=True)\n",
    "\n",
    "# è¼‰å…¥è©é¨™é—œéµå­—\n",
    "keywords_df = dfs[\"keywords\"]\n",
    "scam_keywords = keywords_df[\"è©èª\"].dropna().tolist()\n",
    "\n",
    "# çµåˆæ¬„ä½æ–‡æœ¬\n",
    "def combine_text(row):\n",
    "    content = ''\n",
    "    for col in ['ç™¼æ–‡å…§å®¹', 'ç•™è¨€å…§å®¹', 'content']:\n",
    "        if col in row and pd.notna(row[col]):\n",
    "            content += str(row[col]) + ' '\n",
    "    return content.strip()\n",
    "\n",
    "combined_df['text'] = combined_df.apply(combine_text, axis=1)\n",
    "\n",
    "# æ¨™è¨˜æ˜¯å¦ç‚ºè©é¨™å¥å­\n",
    "def is_scam(text):\n",
    "    return int(any(kw in text for kw in scam_keywords))\n",
    "\n",
    "combined_df['label'] = combined_df['text'].apply(is_scam)\n",
    "\n",
    "# ç¯©é™¤ç©ºæ–‡å­—\n",
    "combined_df = combined_df[combined_df['text'].str.strip().astype(bool)]\n",
    "\n",
    "# ä¸­æ–‡æ–·è©\n",
    "def tokenize(text):\n",
    "    return \" \".join(jieba.cut(text))\n",
    "\n",
    "combined_df['text_tokenized'] = combined_df['text'].apply(tokenize)\n",
    "\n",
    "# æ‹†åˆ†è³‡æ–™\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    combined_df['text_tokenized'],\n",
    "    combined_df['label'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Tokenizer ç·¨ç¢¼æ–‡å­—\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# è£œé½Šé•·åº¦\n",
    "max_len = 100\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
    "\n",
    "# å»ºç«‹ç°¡å–® LSTM æ¨¡å‹\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=64, input_length=max_len),\n",
    "    LSTM(64, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# è¨“ç·´æ¨¡å‹ï¼ˆç¸®çŸ­ç‚º 3 epochï¼‰\n",
    "model.fit(X_train_pad, y_train, epochs=3, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# å®šç¾©è©é¨™é æ¸¬å‡½æ•¸\n",
    "def predict_scam(sentence):\n",
    "    tokenized = \" \".join(jieba.cut(sentence))\n",
    "    seq = tokenizer.texts_to_sequences([tokenized])\n",
    "    padded = pad_sequences(seq, maxlen=max_len)\n",
    "    prob = model.predict(padded)[0][0]\n",
    "    return prob\n",
    "\n",
    "# ç¯„ä¾‹é æ¸¬\n",
    "example_sentence = \"æ‚¨ä¸­çäº†ï¼è«‹ç«‹å³å›æ’¥å°ˆç·šé ˜ç\"\n",
    "example_prob = predict_scam(example_sentence)\n",
    "example_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39adbb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9997644"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# å®šç¾©è©é¨™é æ¸¬å‡½æ•¸\n",
    "def predict_scam(sentence):\n",
    "    tokenized = \" \".join(jieba.cut(sentence))\n",
    "    seq = tokenizer.texts_to_sequences([tokenized])\n",
    "    padded = pad_sequences(seq, maxlen=max_len)\n",
    "    prob = model.predict(padded)[0][0]\n",
    "    return prob\n",
    "\n",
    "# ç¯„ä¾‹é æ¸¬\n",
    "example_sentence = \"\tæ‚¨çš„å¸³æˆ¶ç•°å¸¸ï¼Œéœ€ç«‹å³æ“ä½œATMè§£é™¤åˆ†æœŸä»˜æ¬¾ã€‚\"\n",
    "example_prob = predict_scam(example_sentence)\n",
    "example_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e55c46cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robby1206\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m953/953\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 52ms/step - accuracy: 0.7586 - loss: 0.4747 - val_accuracy: 0.7833 - val_loss: 0.4257\n",
      "Epoch 2/3\n",
      "\u001b[1m953/953\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 52ms/step - accuracy: 0.8041 - loss: 0.4001 - val_accuracy: 0.7863 - val_loss: 0.4202\n",
      "Epoch 3/3\n",
      "\u001b[1m953/953\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 53ms/step - accuracy: 0.8174 - loss: 0.3747 - val_accuracy: 0.7882 - val_loss: 0.4246\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 344ms/step\n",
      "0.53160834\n"
     ]
    }
   ],
   "source": [
    "# ========== 1. å®‰è£å¥—ä»¶ï¼ˆç¬¬ä¸€æ¬¡ç”¨ï¼‰ ==========\n",
    "# pip install jieba pandas scikit-learn tensorflow openpyxl\n",
    "\n",
    "# ========== 2. è¼‰å…¥å¿…è¦æ¨¡çµ„ ==========\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# ========== 3. è®€å–è³‡æ–™ ==========\n",
    "ptt_df = pd.read_csv(\"DATA_DIR/ptt_èªæ–™_è™•ç†å¾Œ.csv\")\n",
    "mobile_df = pd.read_csv(\"DATA_DIR/mobile01_è™•ç†å¾Œ.csv\")\n",
    "keywords_df = pd.read_csv(\"DATA_DIR/500ç²¾ç°¡è©é¨™å­—è©_UTF8.csv\")\n",
    "scam_keywords = keywords_df[\"è©èª\"].dropna().tolist()\n",
    "\n",
    "df = pd.concat([ptt_df, mobile_df], ignore_index=True)\n",
    "\n",
    "# åˆä½µå…§å®¹æ¬„\n",
    "def combine_text(row):\n",
    "    for col in ['ç™¼æ–‡å…§å®¹', 'ç•™è¨€å…§å®¹', 'content']:\n",
    "        if col in row and pd.notna(row[col]):\n",
    "            return str(row[col])\n",
    "    return \"\"\n",
    "\n",
    "df[\"text\"] = df.apply(combine_text, axis=1)\n",
    "\n",
    "# æ¨™è¨˜è©é¨™èˆ‡å¦\n",
    "df[\"label\"] = df[\"text\"].apply(lambda x: int(any(k in x for k in scam_keywords)))\n",
    "df = df[df[\"text\"].str.strip().astype(bool)]\n",
    "\n",
    "# å»é™¤é—œéµå­—ï¼Œé¿å…è³‡æ–™æ´©æ¼\n",
    "def remove_keywords(text, keywords):\n",
    "    for k in keywords:\n",
    "        text = text.replace(k, \"\")\n",
    "    return text\n",
    "\n",
    "df[\"text_clean\"] = df[\"text\"].apply(lambda x: remove_keywords(x, scam_keywords))\n",
    "df[\"text_tokenized\"] = df[\"text_clean\"].apply(lambda x: \" \".join(jieba.cut(x)))\n",
    "\n",
    "# ========== 4. å»ºç«‹è¨“ç·´é›† ==========\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text_tokenized\"], df[\"label\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# ========== 5. å‘é‡åŒ– ==========\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_len = 100\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
    "\n",
    "# ========== 6. å»ºç«‹æ¨¡å‹ ==========\n",
    "model = Sequential([\n",
    "    Embedding(5000, 64, input_length=max_len),\n",
    "    LSTM(64, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# ========== 7. è¨“ç·´æ¨¡å‹ ==========\n",
    "model.fit(X_train_pad, y_train, epochs=3, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# ========== 8. é æ¸¬å‡½å¼ ==========\n",
    "def predict_scam(sentence):\n",
    "    tokenized = \" \".join(jieba.cut(sentence))\n",
    "    seq = tokenizer.texts_to_sequences([tokenized])\n",
    "    padded = pad_sequences(seq, maxlen=max_len)\n",
    "    prob = model.predict(padded)[0][0]\n",
    "    return prob\n",
    "\n",
    "# æ¸¬è©¦ä¾‹å¥\n",
    "print(predict_scam(\"æ‚¨ä¸­çäº†ï¼è«‹ç«‹å³å›æ’¥å°ˆç·šé ˜ç\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9396cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "         æ‚¨å·²ä¸­çï¼Œè«‹ç«‹å³å›æ’¥å°ˆç·šé ˜çã€‚ã€â€‹      è©é¨™æ©Ÿç‡\n",
      "0   æ‚¨çš„å¸³æˆ¶ç•°å¸¸ï¼Œéœ€ç«‹å³æ“ä½œATMè§£é™¤åˆ†æœŸä»˜æ¬¾ã€‚  0.935278\n",
      "1    æˆ‘æ˜¯æª¢å¯Ÿå®˜ï¼Œæ‚¨çš„å¸³æˆ¶æ¶‰åŠæ´—éŒ¢ï¼Œéœ€é…åˆèª¿æŸ¥ã€‚  0.972129\n",
      "2        åŠ å…¥æˆ‘å€‘çš„æŠ•è³‡è¨ˆç•«ï¼Œä¿è­‰ç©©å®šç²åˆ©ã€‚  0.765351\n",
      "3        æˆ‘å€‘æ˜¯çŸ¥åä¼æ¥­ï¼Œæä¾›é«˜è–ªå…¼è·æ©Ÿæœƒã€‚  0.952347\n",
      "4      æ‚¨çš„å¥ä¿å¡è¢«ç›œç”¨ï¼Œéœ€æä¾›å€‹äººè³‡æ–™æ ¸å°ã€‚  0.973686\n",
      "5     è«‹å…ˆæ”¯ä»˜ä¿è­‰é‡‘ï¼Œæˆ‘å€‘å°‡ç«‹å³è™•ç†æ‚¨çš„å•é¡Œã€‚  0.855302\n",
      "6       æˆ‘å€‘å…¬å¸æœ‰ç ´è§£ç¨‹å¼ï¼Œèƒ½å¹«æ‚¨è¿½å›æå¤±ã€‚  0.956261\n",
      "7        é€™æ˜¯é™æ™‚å„ªæƒ ï¼Œæ¯å¤©åƒ…é™50å€‹åé¡ã€‚  0.641756\n",
      "8   è«‹åŠ å…¥æˆ‘å€‘çš„LINEç¾¤çµ„ï¼Œç­è§£æ›´å¤šè³ºéŒ¢æ©Ÿæœƒã€‚  0.789238\n",
      "9  æ‚¨å¥½ï¼Œæ­¡è¿å…‰è‡¨æœ¬åº—ï¼Œè«‹å•æœ‰ä»€éº¼å¯ä»¥ç‚ºæ‚¨æœå‹™çš„ï¼Ÿ  0.993549\n",
      "âœ… å·²å„²å­˜ç‚º èªå¥è©é¨™é æ¸¬çµæœ.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# è¼‰å…¥ Excel\n",
    "df = pd.read_excel(\"èªå¥é æ¸¬.xlsx\")\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# è‡ªå‹•åˆ¤æ–·å¥å­æ¬„ä½åç¨±\n",
    "if \"å¥å­\" in df.columns:\n",
    "    sentence_col = \"å¥å­\"\n",
    "else:\n",
    "    sentence_col = df.columns[0]  # é è¨­ä½¿ç”¨ç¬¬ä¸€æ¬„\n",
    "\n",
    "# å–å¾—èªå¥åˆ—è¡¨\n",
    "sentences = df[sentence_col].dropna().astype(str).tolist()\n",
    "\n",
    "# é æ¸¬å‡½æ•¸\n",
    "def predict_scam_batch(sent_list):\n",
    "    results = []\n",
    "    for sentence in sent_list:\n",
    "        tokenized = \" \".join(jieba.cut(sentence))\n",
    "        seq = tokenizer.texts_to_sequences([tokenized])\n",
    "        padded = pad_sequences(seq, maxlen=max_len)\n",
    "        prob = model.predict(padded)[0][0]\n",
    "        results.append(prob)\n",
    "    return results\n",
    "\n",
    "# é æ¸¬\n",
    "df[\"è©é¨™æ©Ÿç‡\"] = predict_scam_batch(sentences)\n",
    "\n",
    "# é¡¯ç¤ºçµæœ\n",
    "print(df[[sentence_col, \"è©é¨™æ©Ÿç‡\"]].head(10))\n",
    "\n",
    "# å„²å­˜\n",
    "df.to_excel(\"èªå¥è©é¨™é æ¸¬çµæœ.xlsx\", index=False)\n",
    "print(\"âœ… å·²å„²å­˜ç‚º èªå¥è©é¨™é æ¸¬çµæœ.xlsx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_scraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
